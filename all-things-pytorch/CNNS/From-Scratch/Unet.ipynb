{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8DRfABDtrDAv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torchvision.transforms.functional as TF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_conv_out_shape(height, width, kernel, stride, padding):\n",
        "  new_width = math.floor(((width - kernel + 2 * padding)/stride) + 1)\n",
        "  new_height = math.floor(((height - kernel + 2 * padding)/stride) + 1)\n",
        "\n",
        "  return new_width, new_height"
      ],
      "metadata": {
        "id": "HCGHsnfErHyt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calculate_conv_out_shape(572, 572, 3, 1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9qd49WY96wa",
        "outputId": "94b664ba-7270-4776-9b31-853f6953cacf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(572, 572)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(DoubleConv, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchNorm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.batchNorm2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(self.batchNorm1(x))\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(self.batchNorm2(x))\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "LjcNHLzRrJX0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, model_channels=[64, 128, 256, 512]):\n",
        "    super(Unet, self).__init__()\n",
        "    self.input_channel = in_channels\n",
        "    self.output_channel = out_channels\n",
        "    self.encoder = nn.ModuleList()\n",
        "    self.decoder = nn.ModuleList()\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2) # halves the 2d tensors\n",
        "    self.final_conv = nn.Conv2d(model_channels[0], out_channels, kernel_size=1) # restoring the channels at the final convolution\n",
        "\n",
        "    # encoder part\n",
        "    for channel in model_channels:\n",
        "      self.encoder.append(DoubleConv(self.input_channel, channel))\n",
        "      self.input_channel = channel\n",
        "      print(self.input_channel)\n",
        "\n",
        "    self.bottleneck = DoubleConv(model_channels[-1], model_channels[-1]*2) # Channels: 512 -> 1024\n",
        "\n",
        "    # decoder part\n",
        "    for channel in reversed(model_channels):\n",
        "      self.decoder.append(nn.ConvTranspose2d(channel * 2, channel, kernel_size=2, stride=2)) # (B, 256, 64, 64) -> (B, 128, 128, 128)\n",
        "      self.decoder.append(DoubleConv(channel * 2, channel))\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    skip_connections = []\n",
        "\n",
        "    for encoder_layer in self.encoder:\n",
        "      x = encoder_layer(x)\n",
        "      print('After one enc layer: ', x.shape)\n",
        "      skip_connections.append(x)\n",
        "      x = self.pool(x)\n",
        "      print('After one maxpool layer: ', x.shape)\n",
        "\n",
        "    x = self.bottleneck(x)\n",
        "    print('After bottleneck: ', x.shape)\n",
        "    print()\n",
        "    skip_connections = skip_connections[::-1] # reversing the skip connection list\n",
        "\n",
        "    print(len(self.decoder))\n",
        "    print(len(skip_connections))\n",
        "\n",
        "    for idx in range(0, len(self.decoder), 2):\n",
        "      print('Before:', x.shape)\n",
        "      x = self.decoder[idx](x) # transpose conv\n",
        "      skip_connection = skip_connections[idx//2] # concatenation\n",
        "\n",
        "      print('After x shape: ', x.shape)\n",
        "      print('skip_connection shape: ', skip_connection.shape)\n",
        "\n",
        "      if x.shape != skip_connection.shape:\n",
        "        x = TF.resize(x, (skip_connection.shape[2], skip_connection.shape[3]))\n",
        "\n",
        "      x = torch.cat((x, skip_connection),dim=1) # (B, 512, 64, 64) + (B, 512, 64, 64) = (B, 1024, 64, 64)\n",
        "      x = self.decoder[idx+1](x) # double conv\n",
        "\n",
        "    return self.final_conv(x) # final convolution to desired segmentation mask\n"
      ],
      "metadata": {
        "id": "X2shgScCrKwi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    net = DoubleConv(3, 1024, 1, 1)\n",
        "    print(net)\n",
        "    y = net(torch.randn(4, 3, 256, 256)).to(\"cuda\")\n",
        "    print(y.size())\n",
        "    print(y)\n",
        "\n",
        "test()"
      ],
      "metadata": {
        "id": "1AagvEiprMCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    x = torch.randn((3, 1, 161, 161))\n",
        "    model = UNET(in_channels=1, out_channels=1)\n",
        "    preds = model(x)\n",
        "    assert preds.shape == x.shape\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDGAAuQy1r4R",
        "outputId": "3d4e9011-60ee-4f7d-8b7b-7d80dfcd1795"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "128\n",
            "256\n",
            "512\n",
            "After one enc layer:  torch.Size([3, 64, 161, 161])\n",
            "After one maxpool layer:  torch.Size([3, 64, 80, 80])\n",
            "After one enc layer:  torch.Size([3, 128, 80, 80])\n",
            "After one maxpool layer:  torch.Size([3, 128, 40, 40])\n",
            "After one enc layer:  torch.Size([3, 256, 40, 40])\n",
            "After one maxpool layer:  torch.Size([3, 256, 20, 20])\n",
            "After one enc layer:  torch.Size([3, 512, 20, 20])\n",
            "After one maxpool layer:  torch.Size([3, 512, 10, 10])\n",
            "After bottleneck:  torch.Size([3, 1024, 10, 10])\n",
            "\n",
            "8\n",
            "4\n",
            "Before: torch.Size([3, 1024, 10, 10])\n",
            "After x shape:  torch.Size([3, 512, 20, 20])\n",
            "skip_connection shape:  torch.Size([3, 512, 20, 20])\n",
            "Before: torch.Size([3, 512, 20, 20])\n",
            "After x shape:  torch.Size([3, 256, 40, 40])\n",
            "skip_connection shape:  torch.Size([3, 256, 40, 40])\n",
            "Before: torch.Size([3, 256, 40, 40])\n",
            "After x shape:  torch.Size([3, 128, 80, 80])\n",
            "skip_connection shape:  torch.Size([3, 128, 80, 80])\n",
            "Before: torch.Size([3, 128, 80, 80])\n",
            "After x shape:  torch.Size([3, 64, 160, 160])\n",
            "skip_connection shape:  torch.Size([3, 64, 161, 161])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn((3, 64, 160, 160))\n",
        "skip = torch.randn((3, 64, 161, 161))\n",
        "\n",
        "x = TF.resize(x, (skip.shape[2], skip.shape[3]))\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7eKYcblHHD5",
        "outputId": "d4552e6a-9e82-492a-e3a3-8db52f4f2bd3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 64, 161, 161])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.cat((x, skip), dim=1)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KrDO7KeIzVF",
        "outputId": "f38f810b-7de2-4e59-dde6-4ffabc7a7d39"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 128, 161, 161])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "soW_LLw-J9zm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}