{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "_CudaDeviceProperties(name='NVIDIA GeForce RTX 3060 Laptop GPU', major=8, minor=6, total_memory=6143MB, multi_processor_count=30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cyclo\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\cuda\\__init__.py:106: UserWarning: \n",
      "NVIDIA GeForce RTX 3060 Laptop GPU with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\n",
      "If you want to use the NVIDIA GeForce RTX 3060 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_properties(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample elements from a tensor according to an index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7732, 0.6245, 0.6177, 0.7095, 0.4967, 0.3305],\n",
      "         [0.8608, 0.9851, 0.0883, 0.4492, 0.0918, 0.6323],\n",
      "         [0.4553, 0.7353, 0.0564, 0.9996, 0.1318, 0.8802],\n",
      "         [0.4988, 0.5071, 0.9484, 0.0214, 0.7584, 0.6992]],\n",
      "\n",
      "        [[0.7645, 0.1428, 0.4088, 0.9709, 0.8812, 0.2488],\n",
      "         [0.6797, 0.3282, 0.0836, 0.7994, 0.8722, 0.3875],\n",
      "         [0.6713, 0.7480, 0.0142, 0.7383, 0.1143, 0.9689],\n",
      "         [0.4459, 0.3180, 0.6842, 0.5005, 0.4442, 0.8198]],\n",
      "\n",
      "        [[0.4057, 0.3338, 0.6691, 0.2661, 0.9333, 0.7693],\n",
      "         [0.4836, 0.4749, 0.7373, 0.8750, 0.5150, 0.5568],\n",
      "         [0.6588, 0.5423, 0.7297, 0.6539, 0.2321, 0.2306],\n",
      "         [0.7126, 0.4548, 0.3563, 0.5909, 0.2945, 0.5894]],\n",
      "\n",
      "        [[0.1208, 0.0192, 0.3227, 0.9484, 0.4912, 0.0549],\n",
      "         [0.7016, 0.8218, 0.9094, 0.2675, 0.1631, 0.5578],\n",
      "         [0.9397, 0.8473, 0.4838, 0.2945, 0.1123, 0.2284],\n",
      "         [0.0301, 0.7590, 0.4776, 0.9573, 0.3088, 0.6763]]])\n",
      "tensor([[[0, 0, 0, 0, 2, 3],\n",
      "         [3, 3, 0, 1, 2, 3],\n",
      "         [1, 0, 2, 3, 3, 2],\n",
      "         [1, 3, 0, 2, 3, 2]]])\n"
     ]
    }
   ],
   "source": [
    "dim = 0 \n",
    "src = torch.rand(size=(4,4,6))\n",
    "index = torch.randint(low=0, high=4, size=(1,4,6)) # take 1 sample from the first dimension of the source\n",
    "\n",
    "print(src)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.7732, 0.6245, 0.6177, 0.7095, 0.9333, 0.0549],\n",
       "         [0.7016, 0.8218, 0.0883, 0.7994, 0.5150, 0.5578],\n",
       "         [0.6713, 0.7353, 0.7297, 0.2945, 0.1123, 0.2306],\n",
       "         [0.4459, 0.7590, 0.9484, 0.5909, 0.3088, 0.5894]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.gather(src, dim=dim, index=index)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4263, 0.4719, 0.4966, 0.2807, 0.3755, 0.6722],\n",
      "         [0.3608, 0.5857, 0.4039, 0.1472, 0.2629, 0.9168],\n",
      "         [0.5907, 0.3213, 0.7899, 0.8290, 0.6276, 0.2792],\n",
      "         [0.4865, 0.5083, 0.2249, 0.7975, 0.0740, 0.2717]],\n",
      "\n",
      "        [[0.7884, 0.9972, 0.8089, 0.2065, 0.8891, 0.9800],\n",
      "         [0.9294, 0.8191, 0.8146, 0.8348, 0.7782, 0.4272],\n",
      "         [0.7111, 0.6783, 0.2878, 0.9863, 0.5949, 0.4444],\n",
      "         [0.0780, 0.3567, 0.5677, 0.1974, 0.1796, 0.0528]],\n",
      "\n",
      "        [[0.5677, 0.0915, 0.0676, 0.3772, 0.2507, 0.7413],\n",
      "         [0.4854, 0.7301, 0.1869, 0.9655, 0.8660, 0.5788],\n",
      "         [0.7264, 0.4512, 0.3676, 0.7587, 0.8077, 0.4493],\n",
      "         [0.6276, 0.8846, 0.0528, 0.5345, 0.3325, 0.2420]],\n",
      "\n",
      "        [[0.8591, 0.0774, 0.2659, 0.2792, 0.5996, 0.0339],\n",
      "         [0.1537, 0.6730, 0.2564, 0.7447, 0.1327, 0.7947],\n",
      "         [0.7347, 0.1006, 0.3350, 0.1412, 0.4719, 0.2814],\n",
      "         [0.4658, 0.1806, 0.0905, 0.6119, 0.6417, 0.5949]]])\n",
      "tensor([[[1, 3, 2, 3, 1, 2],\n",
      "         [2, 0, 0, 0, 0, 1],\n",
      "         [3, 3, 2, 0, 3, 1],\n",
      "         [1, 2, 0, 0, 1, 0],\n",
      "         [3, 3, 2, 3, 1, 2],\n",
      "         [3, 1, 2, 3, 3, 1],\n",
      "         [2, 2, 1, 2, 0, 2],\n",
      "         [0, 1, 2, 3, 3, 2],\n",
      "         [2, 1, 1, 0, 1, 3],\n",
      "         [3, 1, 1, 2, 3, 3]],\n",
      "\n",
      "        [[0, 1, 3, 1, 1, 2],\n",
      "         [1, 3, 3, 3, 2, 3],\n",
      "         [2, 2, 0, 1, 2, 3],\n",
      "         [1, 0, 1, 3, 2, 1],\n",
      "         [2, 0, 2, 1, 0, 1],\n",
      "         [0, 3, 0, 0, 2, 2],\n",
      "         [1, 2, 0, 0, 2, 3],\n",
      "         [1, 0, 0, 3, 2, 2],\n",
      "         [3, 3, 0, 1, 3, 3],\n",
      "         [1, 3, 0, 1, 1, 3]],\n",
      "\n",
      "        [[3, 2, 0, 0, 1, 1],\n",
      "         [2, 2, 1, 1, 3, 1],\n",
      "         [3, 3, 2, 1, 1, 0],\n",
      "         [0, 0, 0, 3, 2, 0],\n",
      "         [2, 0, 0, 3, 2, 2],\n",
      "         [1, 0, 0, 3, 1, 1],\n",
      "         [1, 2, 2, 0, 1, 2],\n",
      "         [3, 2, 0, 3, 2, 2],\n",
      "         [1, 1, 2, 2, 0, 3],\n",
      "         [3, 0, 3, 3, 2, 1]],\n",
      "\n",
      "        [[0, 3, 1, 2, 0, 1],\n",
      "         [3, 3, 3, 3, 1, 0],\n",
      "         [3, 3, 1, 1, 1, 0],\n",
      "         [0, 1, 2, 3, 2, 0],\n",
      "         [1, 2, 0, 1, 2, 2],\n",
      "         [1, 3, 1, 1, 1, 0],\n",
      "         [3, 2, 0, 2, 1, 2],\n",
      "         [3, 1, 0, 1, 3, 1],\n",
      "         [3, 2, 3, 2, 1, 2],\n",
      "         [3, 2, 2, 1, 0, 0]]])\n"
     ]
    }
   ],
   "source": [
    "# take 10 sample from the second dimension of the source\n",
    "# simply take the element at a particular index\n",
    "dim = 1\n",
    "src = torch.rand(size=(4, 4, 6))\n",
    "index = torch.randint(low=0, high=4, size=(4, 10, 6)) \n",
    "\n",
    "print(src)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3608, 0.5083, 0.7899, 0.7975, 0.2629, 0.2792],\n",
       "         [0.5907, 0.4719, 0.4966, 0.2807, 0.3755, 0.9168],\n",
       "         [0.4865, 0.5083, 0.7899, 0.2807, 0.0740, 0.9168],\n",
       "         [0.3608, 0.3213, 0.4966, 0.2807, 0.2629, 0.6722],\n",
       "         [0.4865, 0.5083, 0.7899, 0.7975, 0.2629, 0.2792],\n",
       "         [0.4865, 0.5857, 0.7899, 0.7975, 0.0740, 0.9168],\n",
       "         [0.5907, 0.3213, 0.4039, 0.8290, 0.3755, 0.2792],\n",
       "         [0.4263, 0.5857, 0.7899, 0.7975, 0.0740, 0.2792],\n",
       "         [0.5907, 0.5857, 0.4039, 0.2807, 0.2629, 0.2717],\n",
       "         [0.4865, 0.5857, 0.4039, 0.8290, 0.0740, 0.2717]],\n",
       "\n",
       "        [[0.7884, 0.8191, 0.5677, 0.8348, 0.7782, 0.4444],\n",
       "         [0.9294, 0.3567, 0.5677, 0.1974, 0.5949, 0.0528],\n",
       "         [0.7111, 0.6783, 0.8089, 0.8348, 0.5949, 0.0528],\n",
       "         [0.9294, 0.9972, 0.8146, 0.1974, 0.5949, 0.4272],\n",
       "         [0.7111, 0.9972, 0.2878, 0.8348, 0.8891, 0.4272],\n",
       "         [0.7884, 0.3567, 0.8089, 0.2065, 0.5949, 0.4444],\n",
       "         [0.9294, 0.6783, 0.8089, 0.2065, 0.5949, 0.0528],\n",
       "         [0.9294, 0.9972, 0.8089, 0.1974, 0.5949, 0.4444],\n",
       "         [0.0780, 0.3567, 0.8089, 0.8348, 0.1796, 0.0528],\n",
       "         [0.9294, 0.3567, 0.8089, 0.8348, 0.7782, 0.0528]],\n",
       "\n",
       "        [[0.6276, 0.4512, 0.0676, 0.3772, 0.8660, 0.5788],\n",
       "         [0.7264, 0.4512, 0.1869, 0.9655, 0.3325, 0.5788],\n",
       "         [0.6276, 0.8846, 0.3676, 0.9655, 0.8660, 0.7413],\n",
       "         [0.5677, 0.0915, 0.0676, 0.5345, 0.8077, 0.7413],\n",
       "         [0.7264, 0.0915, 0.0676, 0.5345, 0.8077, 0.4493],\n",
       "         [0.4854, 0.0915, 0.0676, 0.5345, 0.8660, 0.5788],\n",
       "         [0.4854, 0.4512, 0.3676, 0.3772, 0.8660, 0.4493],\n",
       "         [0.6276, 0.4512, 0.0676, 0.5345, 0.8077, 0.4493],\n",
       "         [0.4854, 0.7301, 0.3676, 0.7587, 0.2507, 0.2420],\n",
       "         [0.6276, 0.0915, 0.0528, 0.5345, 0.8077, 0.5788]],\n",
       "\n",
       "        [[0.8591, 0.1806, 0.2564, 0.1412, 0.5996, 0.7947],\n",
       "         [0.4658, 0.1806, 0.0905, 0.6119, 0.1327, 0.0339],\n",
       "         [0.4658, 0.1806, 0.2564, 0.7447, 0.1327, 0.0339],\n",
       "         [0.8591, 0.6730, 0.3350, 0.6119, 0.4719, 0.0339],\n",
       "         [0.1537, 0.1006, 0.2659, 0.7447, 0.4719, 0.2814],\n",
       "         [0.1537, 0.1806, 0.2564, 0.7447, 0.1327, 0.0339],\n",
       "         [0.4658, 0.1006, 0.2659, 0.1412, 0.1327, 0.2814],\n",
       "         [0.4658, 0.6730, 0.2659, 0.7447, 0.6417, 0.7947],\n",
       "         [0.4658, 0.1006, 0.0905, 0.1412, 0.1327, 0.2814],\n",
       "         [0.4658, 0.1006, 0.3350, 0.7447, 0.5996, 0.0339]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = torch.gather(src, dim=dim, index=index)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = src.zeros_like(size=(4,10,6))\n",
    "for i in range(4):\n",
    "    for j in range(10): # sampling 10 values\n",
    "        for k in range(6):\n",
    "            out[i, j, k] = src[i, index[i, j, k], k]\n",
    "\n",
    "# so only more values are taken according to the index from the desired dimension\n",
    "# the other index are similar to the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10]])\n",
      "tensor([[0, 1, 2, 2]]) torch.Size([1, 4])\n",
      "tensor([[1, 0, 0, 0, 0],\n",
      "        [0, 2, 0, 0, 0],\n",
      "        [0, 0, 3, 4, 0]])\n",
      "tensor([[1, 2, 4, 0, 0],\n",
      "        [0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "src = torch.arange(1, 11).reshape((2, 5))\n",
    "print(src)\n",
    "index = torch.tensor([[0, 1, 2, 2]])\n",
    "print(index, index.shape)\n",
    "print(torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src))\n",
    "print(torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5360, 0.7508, 0.2156, 0.4946, 0.9327, 0.9940],\n",
      "         [0.8418, 0.6638, 0.6995, 0.0971, 0.9880, 0.7936],\n",
      "         [0.1779, 0.0592, 0.9461, 0.2677, 0.5582, 0.2908],\n",
      "         [0.9603, 0.7070, 0.1831, 0.2145, 0.1249, 0.7105]]])\n",
      "tensor([[[1, 1, 3, 2, 1, 2],\n",
      "         [1, 1, 2, 3, 0, 2],\n",
      "         [1, 3, 0, 3, 0, 0],\n",
      "         [3, 0, 3, 0, 1, 0]]])\n"
     ]
    }
   ],
   "source": [
    "dim = 0\n",
    "src = torch.rand(size=(1, 4, 6))\n",
    "index = torch.randint(low=0, high=4, size=(1, 4, 6)) \n",
    "\n",
    "print(src)\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "zeros_like() missing 1 required positional arguments: \"input\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25308/1967526496.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: zeros_like() missing 1 required positional arguments: \"input\""
     ]
    }
   ],
   "source": [
    "out = torch.zeros_like(size=(4,4,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.9239, 0.7473, 2.4038, 1.4481, 3.7873, 2.5246],\n",
       "         [1.0870, 1.6130, 1.4064, 0.9872, 2.5785, 1.9758],\n",
       "         [1.2082, 1.1183, 2.6432, 0.5124, 0.6321, 3.2752],\n",
       "         [2.6070, 1.7724, 3.4063, 2.0309, 2.6161, 5.3388],\n",
       "         [0.4865, 0.5083, 0.7899, 0.7975, 0.2629, 0.2792],\n",
       "         [0.4865, 0.5857, 0.7899, 0.7975, 0.0740, 0.9168],\n",
       "         [0.5907, 0.3213, 0.4039, 0.8290, 0.3755, 0.2792],\n",
       "         [0.4263, 0.5857, 0.7899, 0.7975, 0.0740, 0.2792],\n",
       "         [0.5907, 0.5857, 0.4039, 0.2807, 0.2629, 0.2717],\n",
       "         [0.4865, 0.5857, 0.4039, 0.8290, 0.0740, 0.2717]],\n",
       "\n",
       "        [[1.3244, 1.5700, 3.9581, 1.5048, 1.7109, 0.4444],\n",
       "         [2.1120, 4.1197, 0.5677, 0.1974, 1.2159, 0.0528],\n",
       "         [2.5598, 0.7395, 1.3965, 2.2615, 2.9507, 0.0528],\n",
       "         [0.9294, 1.2009, 0.8146, 0.1974, 3.0243, 0.7622],\n",
       "         [0.7111, 0.9972, 0.2878, 0.8348, 0.8891, 0.4272],\n",
       "         [0.7884, 0.3567, 0.8089, 0.2065, 0.5949, 0.4444],\n",
       "         [0.9294, 0.6783, 0.8089, 0.2065, 0.5949, 0.0528],\n",
       "         [0.9294, 0.9972, 0.8089, 0.1974, 0.5949, 0.4444],\n",
       "         [0.0780, 0.3567, 0.8089, 0.8348, 0.1796, 0.0528],\n",
       "         [0.9294, 0.3567, 0.8089, 0.8348, 0.7782, 0.0528]],\n",
       "\n",
       "        [[0.6276, 0.4512, 0.0676, 0.8719, 0.8660, 2.0675],\n",
       "         [0.7264, 0.4512, 0.8864, 1.3091, 0.3325, 1.9080],\n",
       "         [1.4175, 1.2059, 1.1484, 2.2484, 1.5129, 0.7413],\n",
       "         [1.1554, 0.6395, 0.7040, 0.5345, 0.8077, 0.7413],\n",
       "         [0.7264, 0.0915, 0.0676, 0.5345, 0.8077, 0.4493],\n",
       "         [0.4854, 0.0915, 0.0676, 0.5345, 0.8660, 0.5788],\n",
       "         [0.4854, 0.4512, 0.3676, 0.3772, 0.8660, 0.4493],\n",
       "         [0.6276, 0.4512, 0.0676, 0.5345, 0.8077, 0.4493],\n",
       "         [0.4854, 0.7301, 0.3676, 0.7587, 0.2507, 0.2420],\n",
       "         [0.6276, 0.0915, 0.0528, 0.5345, 0.8077, 0.5788]],\n",
       "\n",
       "        [[1.5591, 2.1262, 0.4719, 0.6248, 0.5996, 0.9720],\n",
       "         [3.5844, 1.5465, 1.3103, 1.0416, 2.0888, 0.0561],\n",
       "         [0.7407, 1.2353, 0.2564, 1.0124, 1.0941, 0.0339],\n",
       "         [1.8194, 0.6730, 0.5182, 0.6119, 0.4719, 0.1205],\n",
       "         [0.1537, 0.1006, 0.2659, 0.7447, 0.4719, 0.2814],\n",
       "         [0.1537, 0.1806, 0.2564, 0.7447, 0.1327, 0.0339],\n",
       "         [0.4658, 0.1006, 0.2659, 0.1412, 0.1327, 0.2814],\n",
       "         [0.4658, 0.6730, 0.2659, 0.7447, 0.6417, 0.7947],\n",
       "         [0.4658, 0.1006, 0.0905, 0.1412, 0.1327, 0.2814],\n",
       "         [0.4658, 0.1006, 0.3350, 0.7447, 0.5996, 0.0339]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.scatter_add_(dim=dim, index=index, src=src)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fold and Unfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 6])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.rand(size=(2,3,3,4))\n",
    "\n",
    "# extract blocks\n",
    "blocks = F.unfold(image, kernel_size=2)\n",
    "blocks.shape # batch size, channel * h_ * w_ , num_of_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 6])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view channels as a dimension\n",
    "# shape: (batch, channels, kernel_size, num_blocks)\n",
    "blocks = blocks.view(blocks.size(0), image.size(1), -1, blocks.size(-1))\n",
    "blocks.shape # batch, channels, h_ * w_, num_of_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25308/4206179633.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# fold the blocks back to an image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\cyclo\\anaconda3\\envs\\ai\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mfold\u001b[1;34m(input, output_size, kernel_size, dilation, padding, stride)\u001b[0m\n\u001b[0;32m   4495\u001b[0m             \u001b[0mfold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4496\u001b[0m         )\n\u001b[1;32m-> 4497\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4498\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"{} must be int or 2-tuple for 3D input\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4499\u001b[0m         \u001b[0massert_int_or_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_size\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "blocks = torch.rand(size=(2, 3 * 2 * 2, 2 * 3))\n",
    "output_size = (3,4)\n",
    "\n",
    "# fold the blocks back to an image\n",
    "output = F.fold(output_size, blocks, kernel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a130be47778c8b0b395084eb2ff2c730ca43d4ba8c6593667b6bbb9e5c36be9b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
