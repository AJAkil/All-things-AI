{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "char-rnn.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn7oZK42Ydeb"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kQldp8wYdel"
      },
      "source": [
        "## Load In Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOJ_2I-TYdem"
      },
      "source": [
        "with open('data/anna.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6n7G7hjAYdem",
        "outputId": "147678f7-d9ed-4fdf-99fc-f45a510681a7"
      },
      "source": [
        "text[:100]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNnV5vi1Ydeo"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubBeWoAvYdeo"
      },
      "source": [
        "chars = tuple(set(text))\n",
        "int2char = dict(enumerate(chars))\n",
        "char2int = {ch: integer for integer, ch in int2char.items()}\n",
        "\n",
        "encoded = np.array([char2int[ch] for ch in text])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCm9ofX6Ydep",
        "outputId": "affec935-7912-440e-a838-939f949e4ef3"
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3, 72, 63, 47, 67, 57, 65, 23, 66, 35, 35, 35,  0, 63, 47, 47, 45,\n",
              "       23, 43, 63, 33, 30, 68, 30, 57, 46, 23, 63, 65, 57, 23, 63, 68, 68,\n",
              "       23, 63, 68, 30, 24, 57, 56, 23, 57, 25, 57, 65, 45, 23, 44, 52, 72,\n",
              "       63, 47, 47, 45, 23, 43, 63, 33, 30, 68, 45, 23, 30, 46, 23, 44, 52,\n",
              "       72, 63, 47, 47, 45, 23, 30, 52, 23, 30, 67, 46, 23, 40, 39, 52, 35,\n",
              "       39, 63, 45, 76, 35, 35, 54, 25, 57, 65, 45, 67, 72, 30, 52])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abRWAv1hYdep"
      },
      "source": [
        "## One hot encoding the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqM8LN-jYdep"
      },
      "source": [
        "def one_hot_encode(arr, num_labels):\n",
        "    # declare the one hot array\n",
        "    one_hot = np.zeros((arr.size, num_labels), dtype=np.float32)\n",
        "    # print(one_hot.shape)\n",
        "    \n",
        "    # fill up the appropriate place with 1\n",
        "    # print(np.arange(one_hot.shape[0]))\n",
        "    # print(arr.flatten())\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, num_labels))\n",
        "    # print(one_hot.shape)\n",
        "    return one_hot"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXeUxwzuYdeq",
        "outputId": "a0db5707-0e55-41d8-8c73-8a228a0b99d9"
      },
      "source": [
        "# check that the function works as expected\n",
        "test_seq = np.array([[2, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0YMXRTRYdeq",
        "outputId": "812aadaf-f5ee-4992-d36a-5777eaffe8ad"
      },
      "source": [
        "a = np.arange(12)\n",
        "a.reshape(2,-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  1,  2,  3,  4,  5],\n",
              "       [ 6,  7,  8,  9, 10, 11]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IAY-MS6Yder"
      },
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    \n",
        "    batch_size_total = batch_size * seq_length\n",
        "    # Get the number of batches we can make\n",
        "    n_batches = len(arr)//batch_size_total\n",
        "    \n",
        "    # Keep only enough characters to make full batches\n",
        "    arr = arr[: n_batches * batch_size_total]\n",
        "    \n",
        "    # Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size, -1))\n",
        "    #print(arr)\n",
        "    \n",
        "    # Iterating over the batches using a window of size seq_length\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        \n",
        "        yield x, y"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0_d41HfYdes"
      },
      "source": [
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo5wZmHtYdes",
        "outputId": "a0c01f62-d69d-4b63-e8e4-05232339e1d4"
      },
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[ 3 72 63 47 67 57 65 23 66 35]\n",
            " [46 40 52 23 67 72 63 67 23 63]\n",
            " [57 52 49 23 40 65 23 63 23 43]\n",
            " [46 23 67 72 57 23 18 72 30 57]\n",
            " [23 46 63 39 23 72 57 65 23 67]\n",
            " [18 44 46 46 30 40 52 23 63 52]\n",
            " [23 55 52 52 63 23 72 63 49 23]\n",
            " [53 28 68 40 52 46 24 45 76 23]]\n",
            "\n",
            "y\n",
            " [[72 63 47 67 57 65 23 66 35 35]\n",
            " [40 52 23 67 72 63 67 23 63 67]\n",
            " [52 49 23 40 65 23 63 23 43 40]\n",
            " [23 67 72 57 23 18 72 30 57 43]\n",
            " [46 63 39 23 72 57 65 23 67 57]\n",
            " [44 46 46 30 40 52 23 63 52 49]\n",
            " [55 52 52 63 23 72 63 49 23 46]\n",
            " [28 68 40 52 46 24 45 76 23 51]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sb1xUj9_Ydes",
        "outputId": "87c86052-9dd1-4eaf-8f18-9499304f4533"
      },
      "source": [
        "train_on_gpu = torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else:\n",
        "    print('No GPU is available, training on CPU!')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyzVcpE9Ydet"
      },
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, tokens, num_hidden=256, num_layers=2, drop_prob=0.25, lr=0.01):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.lr=lr\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        \n",
        "        # creating the necessary character dictionary\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii,ch in self.int2char.items()}\n",
        "        \n",
        "        # create the layers of the Model\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.LSTM = nn.LSTM(len(self.chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # Dropot layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        # define the fully connected layer\n",
        "        self.fc = nn.Linear(num_hidden, len(self.chars))\n",
        "        \n",
        "    def forward(self, x, hidden):\n",
        "        \n",
        "        r_output, hidden  = self.LSTM(x, hidden)\n",
        "        out = self.dropout(r_output)\n",
        "        # print('before contigous',out.shape)\n",
        "        out = out.contiguous().view(-1, self.num_hidden)\n",
        "        # print('after contigous',out.shape)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        \n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            hiddenstate_cellstate = (\n",
        "            weight.new(self.num_layers, batch_size, self.num_hidden).zero_().cuda(),\n",
        "                weight.new(self.num_layers, batch_size, self.num_hidden).zero_().cuda()\n",
        "            )\n",
        "            \n",
        "        else:\n",
        "            \n",
        "            hiddenstate_cellstate = (\n",
        "            weight.new(self.num_layers, batch_size, self.num_hidden).zero_(),\n",
        "                weight.new(self.num_layers, batch_size, self.num_hidden).zero_()\n",
        "            )\n",
        "            \n",
        "        return hiddenstate_cellstate\n",
        "        "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32ha1AFbYdeu"
      },
      "source": [
        "def train(model, data, epochs=10, batch_size=10, seq_length=50, lr = 0.01, clip=5, val_frac=0.1, print_every= 10):\n",
        "        ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "        '''\n",
        "        # take the model to training mode\n",
        "        model.train()\n",
        "        \n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "        \n",
        "        # creating the training and validation datasets\n",
        "        val_idx = int(len(data) * (1 - val_frac))\n",
        "        train_data, valid_data = data[:val_idx], data[val_idx:]\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            model.cuda() # we take the parameters to the gpu\n",
        "            \n",
        "        counter = 0\n",
        "        num_chars = len(model.chars)\n",
        "        \n",
        "        # the epoch loop starsts\n",
        "        for e in range(epochs):\n",
        "            \n",
        "            # initialize the hidden state\n",
        "            h = model.init_hidden(batch_size)\n",
        "            \n",
        "            # we start the batch loop\n",
        "            for x, y in get_batches(train_data, batch_size, seq_length):\n",
        "                counter += 1\n",
        "                \n",
        "                # first step is to one hot encode the input data\n",
        "                x = one_hot_encode(x, num_chars)\n",
        "                \n",
        "                # we convert the data to pytorch tensor\n",
        "                inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                \n",
        "                # if cuda is available we take the tensors to GPU\n",
        "                if(train_on_gpu):\n",
        "                    inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                \n",
        "                # we set the variable for the hidden state\n",
        "                h = tuple([each.data for each in h])\n",
        "                \n",
        "                \n",
        "                # then we zero out any gradients\n",
        "                model.zero_grad()\n",
        "                \n",
        "                \n",
        "                # get the output from the model\n",
        "                output, h = model(inputs, h)\n",
        "#                 print('output shape after lstm',output.shape)\n",
        "#                 print('target shape',targets.shape)\n",
        "#                 print('taget shape after modification', targets.view(batch_size*seq_length).long().shape)\n",
        "                # calculate the lass\n",
        "                loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "                optimizer.step()\n",
        "                \n",
        "                \n",
        "                # loss stats\n",
        "                if counter % print_every == 0:\n",
        "                    \n",
        "                    # validation loop\n",
        "                    val_losses = []\n",
        "                    \n",
        "                    # going to evaluation mode here\n",
        "                    model.eval()\n",
        "                    \n",
        "                    val_h = model.init_hidden(batch_size)\n",
        "                    \n",
        "                    for x,y in get_batches(valid_data, batch_size, seq_length):\n",
        "                        x = one_hot_encode(x, num_chars)\n",
        "                        \n",
        "                        inputs, targets= torch.from_numpy(x), torch.from_numpy(y)\n",
        "                        \n",
        "                        val_h = tuple([each.data for each in val_h])\n",
        "                        \n",
        "                        if(train_on_gpu):\n",
        "                            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "                        \n",
        "                        output, h = model(inputs, val_h)\n",
        "                        val_loss = criterion(output, targets.view(batch_size * seq_length).long())\n",
        "                        val_losses.append(val_loss.item())\n",
        "                    \n",
        "                    # reseeting to train mode again\n",
        "                    model.train()\n",
        "                    \n",
        "                    \n",
        "                    print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "                    "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73DbgqOgYdez",
        "outputId": "0a36923f-fb5b-4282-fffe-bc990e830ab5"
      },
      "source": [
        "## TODO: set your model hyperparameters\n",
        "# define and print the net\n",
        "n_hidden= 512\n",
        "n_layers= 2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (LSTM): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.25)\n",
            "  (dropout): Dropout(p=0.25, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj0QbQz3Ydez",
        "outputId": "52e69743-0de3-46db-8603-d14f7f794f68"
      },
      "source": [
        "batch_size = 64\n",
        "seq_length = 100\n",
        "n_epochs = 20 # start small if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.2202... Val Loss: 3.2502\n",
            "Epoch: 1/20... Step: 20... Loss: 3.1351... Val Loss: 3.1678\n",
            "Epoch: 1/20... Step: 30... Loss: 3.1204... Val Loss: 3.1526\n",
            "Epoch: 1/20... Step: 40... Loss: 3.1380... Val Loss: 3.1477\n",
            "Epoch: 1/20... Step: 50... Loss: 3.1210... Val Loss: 3.1466\n",
            "Epoch: 1/20... Step: 60... Loss: 3.1316... Val Loss: 3.1458\n",
            "Epoch: 1/20... Step: 70... Loss: 3.1263... Val Loss: 3.1437\n",
            "Epoch: 1/20... Step: 80... Loss: 3.1266... Val Loss: 3.1437\n",
            "Epoch: 1/20... Step: 90... Loss: 3.1041... Val Loss: 3.1368\n",
            "Epoch: 1/20... Step: 100... Loss: 3.1139... Val Loss: 3.1294\n",
            "Epoch: 1/20... Step: 110... Loss: 3.0801... Val Loss: 3.1129\n",
            "Epoch: 1/20... Step: 120... Loss: 3.0545... Val Loss: 3.0712\n",
            "Epoch: 1/20... Step: 130... Loss: 2.9953... Val Loss: 3.0338\n",
            "Epoch: 1/20... Step: 140... Loss: 2.9145... Val Loss: 2.9484\n",
            "Epoch: 1/20... Step: 150... Loss: 2.8168... Val Loss: 2.9281\n",
            "Epoch: 1/20... Step: 160... Loss: 2.7206... Val Loss: 2.7850\n",
            "Epoch: 1/20... Step: 170... Loss: 2.6514... Val Loss: 2.6886\n",
            "Epoch: 1/20... Step: 180... Loss: 2.5633... Val Loss: 2.6333\n",
            "Epoch: 1/20... Step: 190... Loss: 2.5164... Val Loss: 2.5776\n",
            "Epoch: 1/20... Step: 200... Loss: 2.5274... Val Loss: 2.5388\n",
            "Epoch: 1/20... Step: 210... Loss: 2.4391... Val Loss: 2.5040\n",
            "Epoch: 1/20... Step: 220... Loss: 2.4750... Val Loss: 2.4753\n",
            "Epoch: 1/20... Step: 230... Loss: 2.3911... Val Loss: 2.4455\n",
            "Epoch: 1/20... Step: 240... Loss: 2.3530... Val Loss: 2.4196\n",
            "Epoch: 1/20... Step: 250... Loss: 2.3313... Val Loss: 2.3975\n",
            "Epoch: 1/20... Step: 260... Loss: 2.3438... Val Loss: 2.3804\n",
            "Epoch: 1/20... Step: 270... Loss: 2.2655... Val Loss: 2.3507\n",
            "Epoch: 2/20... Step: 280... Loss: 2.3134... Val Loss: 2.3370\n",
            "Epoch: 2/20... Step: 290... Loss: 2.2466... Val Loss: 2.3063\n",
            "Epoch: 2/20... Step: 300... Loss: 2.2442... Val Loss: 2.2929\n",
            "Epoch: 2/20... Step: 310... Loss: 2.2225... Val Loss: 2.2639\n",
            "Epoch: 2/20... Step: 320... Loss: 2.1618... Val Loss: 2.2482\n",
            "Epoch: 2/20... Step: 330... Loss: 2.1811... Val Loss: 2.2266\n",
            "Epoch: 2/20... Step: 340... Loss: 2.1422... Val Loss: 2.2151\n",
            "Epoch: 2/20... Step: 350... Loss: 2.1388... Val Loss: 2.1969\n",
            "Epoch: 2/20... Step: 360... Loss: 2.1307... Val Loss: 2.1758\n",
            "Epoch: 2/20... Step: 370... Loss: 2.1138... Val Loss: 2.1615\n",
            "Epoch: 2/20... Step: 380... Loss: 2.0560... Val Loss: 2.1450\n",
            "Epoch: 2/20... Step: 390... Loss: 2.0466... Val Loss: 2.1282\n",
            "Epoch: 2/20... Step: 400... Loss: 2.0135... Val Loss: 2.1139\n",
            "Epoch: 2/20... Step: 410... Loss: 2.0330... Val Loss: 2.1036\n",
            "Epoch: 2/20... Step: 420... Loss: 1.9968... Val Loss: 2.0892\n",
            "Epoch: 2/20... Step: 430... Loss: 1.9929... Val Loss: 2.0791\n",
            "Epoch: 2/20... Step: 440... Loss: 1.9782... Val Loss: 2.0674\n",
            "Epoch: 2/20... Step: 450... Loss: 1.9868... Val Loss: 2.0550\n",
            "Epoch: 2/20... Step: 460... Loss: 1.9637... Val Loss: 2.0348\n",
            "Epoch: 2/20... Step: 470... Loss: 1.9760... Val Loss: 2.0244\n",
            "Epoch: 2/20... Step: 480... Loss: 1.9607... Val Loss: 2.0114\n",
            "Epoch: 2/20... Step: 490... Loss: 1.9028... Val Loss: 2.0026\n",
            "Epoch: 2/20... Step: 500... Loss: 1.9183... Val Loss: 2.0023\n",
            "Epoch: 2/20... Step: 510... Loss: 1.9192... Val Loss: 1.9783\n",
            "Epoch: 2/20... Step: 520... Loss: 1.8960... Val Loss: 1.9719\n",
            "Epoch: 2/20... Step: 530... Loss: 1.8951... Val Loss: 1.9547\n",
            "Epoch: 2/20... Step: 540... Loss: 1.8572... Val Loss: 1.9408\n",
            "Epoch: 2/20... Step: 550... Loss: 1.9225... Val Loss: 1.9368\n",
            "Epoch: 3/20... Step: 560... Loss: 1.8176... Val Loss: 1.9288\n",
            "Epoch: 3/20... Step: 570... Loss: 1.8177... Val Loss: 1.9134\n",
            "Epoch: 3/20... Step: 580... Loss: 1.8266... Val Loss: 1.9001\n",
            "Epoch: 3/20... Step: 590... Loss: 1.7840... Val Loss: 1.8950\n",
            "Epoch: 3/20... Step: 600... Loss: 1.8142... Val Loss: 1.8866\n",
            "Epoch: 3/20... Step: 610... Loss: 1.8227... Val Loss: 1.8773\n",
            "Epoch: 3/20... Step: 620... Loss: 1.7632... Val Loss: 1.8692\n",
            "Epoch: 3/20... Step: 630... Loss: 1.7793... Val Loss: 1.8608\n",
            "Epoch: 3/20... Step: 640... Loss: 1.7453... Val Loss: 1.8516\n",
            "Epoch: 3/20... Step: 650... Loss: 1.7468... Val Loss: 1.8443\n",
            "Epoch: 3/20... Step: 660... Loss: 1.7317... Val Loss: 1.8377\n",
            "Epoch: 3/20... Step: 670... Loss: 1.7445... Val Loss: 1.8281\n",
            "Epoch: 3/20... Step: 680... Loss: 1.7161... Val Loss: 1.8157\n",
            "Epoch: 3/20... Step: 690... Loss: 1.7580... Val Loss: 1.8092\n",
            "Epoch: 3/20... Step: 700... Loss: 1.7582... Val Loss: 1.8034\n",
            "Epoch: 3/20... Step: 710... Loss: 1.7370... Val Loss: 1.7960\n",
            "Epoch: 3/20... Step: 720... Loss: 1.6908... Val Loss: 1.7941\n",
            "Epoch: 3/20... Step: 730... Loss: 1.6885... Val Loss: 1.7808\n",
            "Epoch: 3/20... Step: 740... Loss: 1.7402... Val Loss: 1.7713\n",
            "Epoch: 3/20... Step: 750... Loss: 1.6511... Val Loss: 1.7696\n",
            "Epoch: 3/20... Step: 760... Loss: 1.6681... Val Loss: 1.7681\n",
            "Epoch: 3/20... Step: 770... Loss: 1.6599... Val Loss: 1.7611\n",
            "Epoch: 3/20... Step: 780... Loss: 1.6386... Val Loss: 1.7503\n",
            "Epoch: 3/20... Step: 790... Loss: 1.6873... Val Loss: 1.7463\n",
            "Epoch: 3/20... Step: 800... Loss: 1.6570... Val Loss: 1.7393\n",
            "Epoch: 3/20... Step: 810... Loss: 1.6336... Val Loss: 1.7291\n",
            "Epoch: 3/20... Step: 820... Loss: 1.6461... Val Loss: 1.7267\n",
            "Epoch: 3/20... Step: 830... Loss: 1.5972... Val Loss: 1.7204\n",
            "Epoch: 4/20... Step: 840... Loss: 1.6166... Val Loss: 1.7154\n",
            "Epoch: 4/20... Step: 850... Loss: 1.6116... Val Loss: 1.7089\n",
            "Epoch: 4/20... Step: 860... Loss: 1.6101... Val Loss: 1.7010\n",
            "Epoch: 4/20... Step: 870... Loss: 1.5471... Val Loss: 1.7015\n",
            "Epoch: 4/20... Step: 880... Loss: 1.5747... Val Loss: 1.6953\n",
            "Epoch: 4/20... Step: 890... Loss: 1.5802... Val Loss: 1.6863\n",
            "Epoch: 4/20... Step: 900... Loss: 1.6093... Val Loss: 1.6861\n",
            "Epoch: 4/20... Step: 910... Loss: 1.5357... Val Loss: 1.6791\n",
            "Epoch: 4/20... Step: 920... Loss: 1.5999... Val Loss: 1.6766\n",
            "Epoch: 4/20... Step: 930... Loss: 1.5659... Val Loss: 1.6726\n",
            "Epoch: 4/20... Step: 940... Loss: 1.5498... Val Loss: 1.6650\n",
            "Epoch: 4/20... Step: 950... Loss: 1.5174... Val Loss: 1.6600\n",
            "Epoch: 4/20... Step: 960... Loss: 1.6089... Val Loss: 1.6528\n",
            "Epoch: 4/20... Step: 970... Loss: 1.5726... Val Loss: 1.6500\n",
            "Epoch: 4/20... Step: 980... Loss: 1.5040... Val Loss: 1.6472\n",
            "Epoch: 4/20... Step: 990... Loss: 1.5115... Val Loss: 1.6438\n",
            "Epoch: 4/20... Step: 1000... Loss: 1.5261... Val Loss: 1.6419\n",
            "Epoch: 4/20... Step: 1010... Loss: 1.5469... Val Loss: 1.6360\n",
            "Epoch: 4/20... Step: 1020... Loss: 1.5186... Val Loss: 1.6341\n",
            "Epoch: 4/20... Step: 1030... Loss: 1.5620... Val Loss: 1.6373\n",
            "Epoch: 4/20... Step: 1040... Loss: 1.5160... Val Loss: 1.6277\n",
            "Epoch: 4/20... Step: 1050... Loss: 1.5327... Val Loss: 1.6251\n",
            "Epoch: 4/20... Step: 1060... Loss: 1.5188... Val Loss: 1.6194\n",
            "Epoch: 4/20... Step: 1070... Loss: 1.5128... Val Loss: 1.6131\n",
            "Epoch: 4/20... Step: 1080... Loss: 1.5499... Val Loss: 1.6083\n",
            "Epoch: 4/20... Step: 1090... Loss: 1.4427... Val Loss: 1.6060\n",
            "Epoch: 4/20... Step: 1100... Loss: 1.4948... Val Loss: 1.5998\n",
            "Epoch: 4/20... Step: 1110... Loss: 1.4951... Val Loss: 1.6003\n",
            "Epoch: 5/20... Step: 1120... Loss: 1.4726... Val Loss: 1.5991\n",
            "Epoch: 5/20... Step: 1130... Loss: 1.4813... Val Loss: 1.5931\n",
            "Epoch: 5/20... Step: 1140... Loss: 1.4744... Val Loss: 1.5880\n",
            "Epoch: 5/20... Step: 1150... Loss: 1.4925... Val Loss: 1.5889\n",
            "Epoch: 5/20... Step: 1160... Loss: 1.5076... Val Loss: 1.5858\n",
            "Epoch: 5/20... Step: 1170... Loss: 1.4179... Val Loss: 1.5822\n",
            "Epoch: 5/20... Step: 1180... Loss: 1.4656... Val Loss: 1.5809\n",
            "Epoch: 5/20... Step: 1190... Loss: 1.4380... Val Loss: 1.5755\n",
            "Epoch: 5/20... Step: 1200... Loss: 1.4244... Val Loss: 1.5724\n",
            "Epoch: 5/20... Step: 1210... Loss: 1.4801... Val Loss: 1.5692\n",
            "Epoch: 5/20... Step: 1220... Loss: 1.4317... Val Loss: 1.5681\n",
            "Epoch: 5/20... Step: 1230... Loss: 1.4402... Val Loss: 1.5614\n",
            "Epoch: 5/20... Step: 1240... Loss: 1.4139... Val Loss: 1.5599\n",
            "Epoch: 5/20... Step: 1250... Loss: 1.4464... Val Loss: 1.5518\n",
            "Epoch: 5/20... Step: 1260... Loss: 1.4659... Val Loss: 1.5528\n",
            "Epoch: 5/20... Step: 1270... Loss: 1.4114... Val Loss: 1.5525\n",
            "Epoch: 5/20... Step: 1280... Loss: 1.4416... Val Loss: 1.5493\n",
            "Epoch: 5/20... Step: 1290... Loss: 1.4258... Val Loss: 1.5469\n",
            "Epoch: 5/20... Step: 1300... Loss: 1.4241... Val Loss: 1.5456\n",
            "Epoch: 5/20... Step: 1310... Loss: 1.3615... Val Loss: 1.5433\n",
            "Epoch: 5/20... Step: 1320... Loss: 1.3947... Val Loss: 1.5398\n",
            "Epoch: 5/20... Step: 1330... Loss: 1.4198... Val Loss: 1.5400\n",
            "Epoch: 5/20... Step: 1340... Loss: 1.4359... Val Loss: 1.5371\n",
            "Epoch: 5/20... Step: 1350... Loss: 1.4468... Val Loss: 1.5326\n",
            "Epoch: 5/20... Step: 1360... Loss: 1.4662... Val Loss: 1.5299\n",
            "Epoch: 5/20... Step: 1370... Loss: 1.3901... Val Loss: 1.5293\n",
            "Epoch: 5/20... Step: 1380... Loss: 1.4443... Val Loss: 1.5255\n",
            "Epoch: 5/20... Step: 1390... Loss: 1.3989... Val Loss: 1.5242\n",
            "Epoch: 6/20... Step: 1400... Loss: 1.4128... Val Loss: 1.5199\n",
            "Epoch: 6/20... Step: 1410... Loss: 1.4232... Val Loss: 1.5200\n",
            "Epoch: 6/20... Step: 1420... Loss: 1.4325... Val Loss: 1.5177\n",
            "Epoch: 6/20... Step: 1430... Loss: 1.4109... Val Loss: 1.5199\n",
            "Epoch: 6/20... Step: 1440... Loss: 1.3832... Val Loss: 1.5148\n",
            "Epoch: 6/20... Step: 1450... Loss: 1.3723... Val Loss: 1.5110\n",
            "Epoch: 6/20... Step: 1460... Loss: 1.3999... Val Loss: 1.5101\n",
            "Epoch: 6/20... Step: 1470... Loss: 1.4076... Val Loss: 1.5066\n",
            "Epoch: 6/20... Step: 1480... Loss: 1.3548... Val Loss: 1.5083\n",
            "Epoch: 6/20... Step: 1490... Loss: 1.3924... Val Loss: 1.5039\n",
            "Epoch: 6/20... Step: 1500... Loss: 1.3001... Val Loss: 1.5026\n",
            "Epoch: 6/20... Step: 1510... Loss: 1.2975... Val Loss: 1.5029\n",
            "Epoch: 6/20... Step: 1520... Loss: 1.3865... Val Loss: 1.4997\n",
            "Epoch: 6/20... Step: 1530... Loss: 1.3972... Val Loss: 1.4948\n",
            "Epoch: 6/20... Step: 1540... Loss: 1.3759... Val Loss: 1.4965\n",
            "Epoch: 6/20... Step: 1550... Loss: 1.3829... Val Loss: 1.4951\n",
            "Epoch: 6/20... Step: 1560... Loss: 1.3728... Val Loss: 1.4935\n",
            "Epoch: 6/20... Step: 1570... Loss: 1.4259... Val Loss: 1.4938\n",
            "Epoch: 6/20... Step: 1580... Loss: 1.3376... Val Loss: 1.4900\n",
            "Epoch: 6/20... Step: 1590... Loss: 1.3503... Val Loss: 1.4879\n",
            "Epoch: 6/20... Step: 1600... Loss: 1.3496... Val Loss: 1.4900\n",
            "Epoch: 6/20... Step: 1610... Loss: 1.3547... Val Loss: 1.4876\n",
            "Epoch: 6/20... Step: 1620... Loss: 1.3399... Val Loss: 1.4861\n",
            "Epoch: 6/20... Step: 1630... Loss: 1.3539... Val Loss: 1.4808\n",
            "Epoch: 6/20... Step: 1640... Loss: 1.3423... Val Loss: 1.4818\n",
            "Epoch: 6/20... Step: 1650... Loss: 1.3465... Val Loss: 1.4785\n",
            "Epoch: 6/20... Step: 1660... Loss: 1.3472... Val Loss: 1.4773\n",
            "Epoch: 6/20... Step: 1670... Loss: 1.4033... Val Loss: 1.4775\n",
            "Epoch: 7/20... Step: 1680... Loss: 1.3263... Val Loss: 1.4720\n",
            "Epoch: 7/20... Step: 1690... Loss: 1.3591... Val Loss: 1.4743\n",
            "Epoch: 7/20... Step: 1700... Loss: 1.3055... Val Loss: 1.4714\n",
            "Epoch: 7/20... Step: 1710... Loss: 1.3577... Val Loss: 1.4687\n",
            "Epoch: 7/20... Step: 1720... Loss: 1.3490... Val Loss: 1.4682\n",
            "Epoch: 7/20... Step: 1730... Loss: 1.3019... Val Loss: 1.4671\n",
            "Epoch: 7/20... Step: 1740... Loss: 1.3231... Val Loss: 1.4675\n",
            "Epoch: 7/20... Step: 1750... Loss: 1.3055... Val Loss: 1.4673\n",
            "Epoch: 7/20... Step: 1760... Loss: 1.2676... Val Loss: 1.4673\n",
            "Epoch: 7/20... Step: 1770... Loss: 1.3523... Val Loss: 1.4663\n",
            "Epoch: 7/20... Step: 1780... Loss: 1.2806... Val Loss: 1.4659\n",
            "Epoch: 7/20... Step: 1790... Loss: 1.2884... Val Loss: 1.4647\n",
            "Epoch: 7/20... Step: 1800... Loss: 1.2841... Val Loss: 1.4588\n",
            "Epoch: 7/20... Step: 1810... Loss: 1.3117... Val Loss: 1.4569\n",
            "Epoch: 7/20... Step: 1820... Loss: 1.3007... Val Loss: 1.4567\n",
            "Epoch: 7/20... Step: 1830... Loss: 1.2942... Val Loss: 1.4529\n",
            "Epoch: 7/20... Step: 1840... Loss: 1.2898... Val Loss: 1.4530\n",
            "Epoch: 7/20... Step: 1850... Loss: 1.3194... Val Loss: 1.4537\n",
            "Epoch: 7/20... Step: 1860... Loss: 1.3075... Val Loss: 1.4525\n",
            "Epoch: 7/20... Step: 1870... Loss: 1.2793... Val Loss: 1.4515\n",
            "Epoch: 7/20... Step: 1880... Loss: 1.3100... Val Loss: 1.4554\n",
            "Epoch: 7/20... Step: 1890... Loss: 1.2849... Val Loss: 1.4538\n",
            "Epoch: 7/20... Step: 1900... Loss: 1.2716... Val Loss: 1.4490\n",
            "Epoch: 7/20... Step: 1910... Loss: 1.3119... Val Loss: 1.4464\n",
            "Epoch: 7/20... Step: 1920... Loss: 1.2611... Val Loss: 1.4485\n",
            "Epoch: 7/20... Step: 1930... Loss: 1.2906... Val Loss: 1.4473\n",
            "Epoch: 7/20... Step: 1940... Loss: 1.2828... Val Loss: 1.4454\n",
            "Epoch: 7/20... Step: 1950... Loss: 1.2679... Val Loss: 1.4470\n",
            "Epoch: 8/20... Step: 1960... Loss: 1.2491... Val Loss: 1.4412\n",
            "Epoch: 8/20... Step: 1970... Loss: 1.3119... Val Loss: 1.4420\n",
            "Epoch: 8/20... Step: 1980... Loss: 1.2433... Val Loss: 1.4425\n",
            "Epoch: 8/20... Step: 1990... Loss: 1.3113... Val Loss: 1.4393\n",
            "Epoch: 8/20... Step: 2000... Loss: 1.3312... Val Loss: 1.4367\n",
            "Epoch: 8/20... Step: 2010... Loss: 1.2680... Val Loss: 1.4366\n",
            "Epoch: 8/20... Step: 2020... Loss: 1.2884... Val Loss: 1.4376\n",
            "Epoch: 8/20... Step: 2030... Loss: 1.2582... Val Loss: 1.4373\n",
            "Epoch: 8/20... Step: 2040... Loss: 1.2177... Val Loss: 1.4375\n",
            "Epoch: 8/20... Step: 2050... Loss: 1.3276... Val Loss: 1.4312\n",
            "Epoch: 8/20... Step: 2060... Loss: 1.2478... Val Loss: 1.4358\n",
            "Epoch: 8/20... Step: 2070... Loss: 1.2599... Val Loss: 1.4363\n",
            "Epoch: 8/20... Step: 2080... Loss: 1.2844... Val Loss: 1.4309\n",
            "Epoch: 8/20... Step: 2090... Loss: 1.2751... Val Loss: 1.4267\n",
            "Epoch: 8/20... Step: 2100... Loss: 1.2636... Val Loss: 1.4262\n",
            "Epoch: 8/20... Step: 2110... Loss: 1.2545... Val Loss: 1.4268\n",
            "Epoch: 8/20... Step: 2120... Loss: 1.2901... Val Loss: 1.4300\n",
            "Epoch: 8/20... Step: 2130... Loss: 1.3070... Val Loss: 1.4235\n",
            "Epoch: 8/20... Step: 2140... Loss: 1.2754... Val Loss: 1.4253\n",
            "Epoch: 8/20... Step: 2150... Loss: 1.2762... Val Loss: 1.4257\n",
            "Epoch: 8/20... Step: 2160... Loss: 1.2830... Val Loss: 1.4303\n",
            "Epoch: 8/20... Step: 2170... Loss: 1.2307... Val Loss: 1.4291\n",
            "Epoch: 8/20... Step: 2180... Loss: 1.2685... Val Loss: 1.4256\n",
            "Epoch: 8/20... Step: 2190... Loss: 1.2441... Val Loss: 1.4243\n",
            "Epoch: 8/20... Step: 2200... Loss: 1.2814... Val Loss: 1.4223\n",
            "Epoch: 8/20... Step: 2210... Loss: 1.2839... Val Loss: 1.4198\n",
            "Epoch: 8/20... Step: 2220... Loss: 1.2755... Val Loss: 1.4211\n",
            "Epoch: 8/20... Step: 2230... Loss: 1.2497... Val Loss: 1.4238\n",
            "Epoch: 9/20... Step: 2240... Loss: 1.2380... Val Loss: 1.4170\n",
            "Epoch: 9/20... Step: 2250... Loss: 1.2572... Val Loss: 1.4184\n",
            "Epoch: 9/20... Step: 2260... Loss: 1.2032... Val Loss: 1.4187\n",
            "Epoch: 9/20... Step: 2270... Loss: 1.2462... Val Loss: 1.4160\n",
            "Epoch: 9/20... Step: 2280... Loss: 1.2431... Val Loss: 1.4101\n",
            "Epoch: 9/20... Step: 2290... Loss: 1.2184... Val Loss: 1.4161\n",
            "Epoch: 9/20... Step: 2300... Loss: 1.2123... Val Loss: 1.4162\n",
            "Epoch: 9/20... Step: 2310... Loss: 1.2340... Val Loss: 1.4164\n",
            "Epoch: 9/20... Step: 2320... Loss: 1.2444... Val Loss: 1.4183\n",
            "Epoch: 9/20... Step: 2330... Loss: 1.2215... Val Loss: 1.4116\n",
            "Epoch: 9/20... Step: 2340... Loss: 1.2349... Val Loss: 1.4150\n",
            "Epoch: 9/20... Step: 2350... Loss: 1.2212... Val Loss: 1.4150\n",
            "Epoch: 9/20... Step: 2360... Loss: 1.2685... Val Loss: 1.4090\n",
            "Epoch: 9/20... Step: 2370... Loss: 1.2406... Val Loss: 1.4086\n",
            "Epoch: 9/20... Step: 2380... Loss: 1.2328... Val Loss: 1.4056\n",
            "Epoch: 9/20... Step: 2390... Loss: 1.2224... Val Loss: 1.4059\n",
            "Epoch: 9/20... Step: 2400... Loss: 1.2280... Val Loss: 1.4079\n",
            "Epoch: 9/20... Step: 2410... Loss: 1.2548... Val Loss: 1.4049\n",
            "Epoch: 9/20... Step: 2420... Loss: 1.2232... Val Loss: 1.4024\n",
            "Epoch: 9/20... Step: 2430... Loss: 1.2275... Val Loss: 1.4070\n",
            "Epoch: 9/20... Step: 2440... Loss: 1.2473... Val Loss: 1.4049\n",
            "Epoch: 9/20... Step: 2450... Loss: 1.2483... Val Loss: 1.4071\n",
            "Epoch: 9/20... Step: 2460... Loss: 1.2044... Val Loss: 1.4049\n",
            "Epoch: 9/20... Step: 2470... Loss: 1.2406... Val Loss: 1.4084\n",
            "Epoch: 9/20... Step: 2480... Loss: 1.2091... Val Loss: 1.4046\n",
            "Epoch: 9/20... Step: 2490... Loss: 1.2235... Val Loss: 1.4058\n",
            "Epoch: 9/20... Step: 2500... Loss: 1.1947... Val Loss: 1.4058\n",
            "Epoch: 9/20... Step: 2510... Loss: 1.2018... Val Loss: 1.4059\n",
            "Epoch: 10/20... Step: 2520... Loss: 1.2170... Val Loss: 1.4040\n",
            "Epoch: 10/20... Step: 2530... Loss: 1.2373... Val Loss: 1.4014\n",
            "Epoch: 10/20... Step: 2540... Loss: 1.1962... Val Loss: 1.4048\n",
            "Epoch: 10/20... Step: 2550... Loss: 1.1836... Val Loss: 1.4040\n",
            "Epoch: 10/20... Step: 2560... Loss: 1.1679... Val Loss: 1.4001\n",
            "Epoch: 10/20... Step: 2570... Loss: 1.1586... Val Loss: 1.4012\n",
            "Epoch: 10/20... Step: 2580... Loss: 1.2043... Val Loss: 1.4031\n",
            "Epoch: 10/20... Step: 2590... Loss: 1.2043... Val Loss: 1.3995\n",
            "Epoch: 10/20... Step: 2600... Loss: 1.2022... Val Loss: 1.4024\n",
            "Epoch: 10/20... Step: 2610... Loss: 1.1826... Val Loss: 1.3957\n",
            "Epoch: 10/20... Step: 2620... Loss: 1.1827... Val Loss: 1.3991\n",
            "Epoch: 10/20... Step: 2630... Loss: 1.1599... Val Loss: 1.4038\n",
            "Epoch: 10/20... Step: 2640... Loss: 1.1984... Val Loss: 1.3981\n",
            "Epoch: 10/20... Step: 2650... Loss: 1.1984... Val Loss: 1.3959\n",
            "Epoch: 10/20... Step: 2660... Loss: 1.2010... Val Loss: 1.3902\n",
            "Epoch: 10/20... Step: 2670... Loss: 1.1997... Val Loss: 1.3920\n",
            "Epoch: 10/20... Step: 2680... Loss: 1.2208... Val Loss: 1.3966\n",
            "Epoch: 10/20... Step: 2690... Loss: 1.1831... Val Loss: 1.3931\n",
            "Epoch: 10/20... Step: 2700... Loss: 1.1874... Val Loss: 1.3895\n",
            "Epoch: 10/20... Step: 2710... Loss: 1.2194... Val Loss: 1.3920\n",
            "Epoch: 10/20... Step: 2720... Loss: 1.2208... Val Loss: 1.3918\n",
            "Epoch: 10/20... Step: 2730... Loss: 1.2009... Val Loss: 1.3938\n",
            "Epoch: 10/20... Step: 2740... Loss: 1.2377... Val Loss: 1.3908\n",
            "Epoch: 10/20... Step: 2750... Loss: 1.2057... Val Loss: 1.3964\n",
            "Epoch: 10/20... Step: 2760... Loss: 1.1475... Val Loss: 1.3925\n",
            "Epoch: 10/20... Step: 2770... Loss: 1.1944... Val Loss: 1.3919\n",
            "Epoch: 10/20... Step: 2780... Loss: 1.2309... Val Loss: 1.3917\n",
            "Epoch: 10/20... Step: 2790... Loss: 1.2675... Val Loss: 1.3933\n",
            "Epoch: 11/20... Step: 2800... Loss: 1.1774... Val Loss: 1.3892\n",
            "Epoch: 11/20... Step: 2810... Loss: 1.2167... Val Loss: 1.3878\n",
            "Epoch: 11/20... Step: 2820... Loss: 1.1744... Val Loss: 1.3898\n",
            "Epoch: 11/20... Step: 2830... Loss: 1.1412... Val Loss: 1.3885\n",
            "Epoch: 11/20... Step: 2840... Loss: 1.1891... Val Loss: 1.3860\n",
            "Epoch: 11/20... Step: 2850... Loss: 1.1264... Val Loss: 1.3886\n",
            "Epoch: 11/20... Step: 2860... Loss: 1.2281... Val Loss: 1.3912\n",
            "Epoch: 11/20... Step: 2870... Loss: 1.1697... Val Loss: 1.3882\n",
            "Epoch: 11/20... Step: 2880... Loss: 1.1836... Val Loss: 1.3890\n",
            "Epoch: 11/20... Step: 2890... Loss: 1.1566... Val Loss: 1.3846\n",
            "Epoch: 11/20... Step: 2900... Loss: 1.1889... Val Loss: 1.3858\n",
            "Epoch: 11/20... Step: 2910... Loss: 1.1740... Val Loss: 1.3881\n",
            "Epoch: 11/20... Step: 2920... Loss: 1.1725... Val Loss: 1.3827\n",
            "Epoch: 11/20... Step: 2930... Loss: 1.1370... Val Loss: 1.3854\n",
            "Epoch: 11/20... Step: 2940... Loss: 1.2093... Val Loss: 1.3775\n",
            "Epoch: 11/20... Step: 2950... Loss: 1.1661... Val Loss: 1.3785\n",
            "Epoch: 11/20... Step: 2960... Loss: 1.1955... Val Loss: 1.3823\n",
            "Epoch: 11/20... Step: 2970... Loss: 1.1686... Val Loss: 1.3835\n",
            "Epoch: 11/20... Step: 2980... Loss: 1.1707... Val Loss: 1.3792\n",
            "Epoch: 11/20... Step: 2990... Loss: 1.1884... Val Loss: 1.3824\n",
            "Epoch: 11/20... Step: 3000... Loss: 1.1638... Val Loss: 1.3818\n",
            "Epoch: 11/20... Step: 3010... Loss: 1.1679... Val Loss: 1.3828\n",
            "Epoch: 11/20... Step: 3020... Loss: 1.1385... Val Loss: 1.3808\n",
            "Epoch: 11/20... Step: 3030... Loss: 1.1766... Val Loss: 1.3861\n",
            "Epoch: 11/20... Step: 3040... Loss: 1.1688... Val Loss: 1.3837\n",
            "Epoch: 11/20... Step: 3050... Loss: 1.1797... Val Loss: 1.3826\n",
            "Epoch: 11/20... Step: 3060... Loss: 1.1672... Val Loss: 1.3807\n",
            "Epoch: 12/20... Step: 3070... Loss: 1.2472... Val Loss: 1.3835\n",
            "Epoch: 12/20... Step: 3080... Loss: 1.1825... Val Loss: 1.3822\n",
            "Epoch: 12/20... Step: 3090... Loss: 1.1927... Val Loss: 1.3777\n",
            "Epoch: 12/20... Step: 3100... Loss: 1.1758... Val Loss: 1.3801\n",
            "Epoch: 12/20... Step: 3110... Loss: 1.1240... Val Loss: 1.3812\n",
            "Epoch: 12/20... Step: 3120... Loss: 1.1566... Val Loss: 1.3804\n",
            "Epoch: 12/20... Step: 3130... Loss: 1.1378... Val Loss: 1.3811\n",
            "Epoch: 12/20... Step: 3140... Loss: 1.1451... Val Loss: 1.3815\n",
            "Epoch: 12/20... Step: 3150... Loss: 1.1780... Val Loss: 1.3797\n",
            "Epoch: 12/20... Step: 3160... Loss: 1.1614... Val Loss: 1.3778\n",
            "Epoch: 12/20... Step: 3170... Loss: 1.1555... Val Loss: 1.3766\n",
            "Epoch: 12/20... Step: 3180... Loss: 1.1632... Val Loss: 1.3768\n",
            "Epoch: 12/20... Step: 3190... Loss: 1.1619... Val Loss: 1.3777\n",
            "Epoch: 12/20... Step: 3200... Loss: 1.1730... Val Loss: 1.3772\n",
            "Epoch: 12/20... Step: 3210... Loss: 1.1630... Val Loss: 1.3798\n",
            "Epoch: 12/20... Step: 3220... Loss: 1.1947... Val Loss: 1.3702\n",
            "Epoch: 12/20... Step: 3230... Loss: 1.1418... Val Loss: 1.3723\n",
            "Epoch: 12/20... Step: 3240... Loss: 1.1810... Val Loss: 1.3731\n",
            "Epoch: 12/20... Step: 3250... Loss: 1.1511... Val Loss: 1.3756\n",
            "Epoch: 12/20... Step: 3260... Loss: 1.1741... Val Loss: 1.3697\n",
            "Epoch: 12/20... Step: 3270... Loss: 1.1743... Val Loss: 1.3750\n",
            "Epoch: 12/20... Step: 3280... Loss: 1.1498... Val Loss: 1.3724\n",
            "Epoch: 12/20... Step: 3290... Loss: 1.1657... Val Loss: 1.3741\n",
            "Epoch: 12/20... Step: 3300... Loss: 1.1666... Val Loss: 1.3729\n",
            "Epoch: 12/20... Step: 3310... Loss: 1.1487... Val Loss: 1.3761\n",
            "Epoch: 12/20... Step: 3320... Loss: 1.1477... Val Loss: 1.3763\n",
            "Epoch: 12/20... Step: 3330... Loss: 1.1189... Val Loss: 1.3771\n",
            "Epoch: 12/20... Step: 3340... Loss: 1.2170... Val Loss: 1.3769\n",
            "Epoch: 13/20... Step: 3350... Loss: 1.1462... Val Loss: 1.3750\n",
            "Epoch: 13/20... Step: 3360... Loss: 1.1417... Val Loss: 1.3754\n",
            "Epoch: 13/20... Step: 3370... Loss: 1.1423... Val Loss: 1.3709\n",
            "Epoch: 13/20... Step: 3380... Loss: 1.1068... Val Loss: 1.3747\n",
            "Epoch: 13/20... Step: 3390... Loss: 1.1379... Val Loss: 1.3741\n",
            "Epoch: 13/20... Step: 3400... Loss: 1.1473... Val Loss: 1.3740\n",
            "Epoch: 13/20... Step: 3410... Loss: 1.1045... Val Loss: 1.3736\n",
            "Epoch: 13/20... Step: 3420... Loss: 1.1674... Val Loss: 1.3733\n",
            "Epoch: 13/20... Step: 3430... Loss: 1.1176... Val Loss: 1.3716\n",
            "Epoch: 13/20... Step: 3440... Loss: 1.1293... Val Loss: 1.3716\n",
            "Epoch: 13/20... Step: 3450... Loss: 1.1273... Val Loss: 1.3725\n",
            "Epoch: 13/20... Step: 3460... Loss: 1.1131... Val Loss: 1.3687\n",
            "Epoch: 13/20... Step: 3470... Loss: 1.0932... Val Loss: 1.3736\n",
            "Epoch: 13/20... Step: 3480... Loss: 1.1825... Val Loss: 1.3734\n",
            "Epoch: 13/20... Step: 3490... Loss: 1.1565... Val Loss: 1.3741\n",
            "Epoch: 13/20... Step: 3500... Loss: 1.1460... Val Loss: 1.3669\n",
            "Epoch: 13/20... Step: 3510... Loss: 1.1369... Val Loss: 1.3692\n",
            "Epoch: 13/20... Step: 3520... Loss: 1.1477... Val Loss: 1.3711\n",
            "Epoch: 13/20... Step: 3530... Loss: 1.1513... Val Loss: 1.3705\n",
            "Epoch: 13/20... Step: 3540... Loss: 1.1142... Val Loss: 1.3656\n",
            "Epoch: 13/20... Step: 3550... Loss: 1.1358... Val Loss: 1.3678\n",
            "Epoch: 13/20... Step: 3560... Loss: 1.1039... Val Loss: 1.3674\n",
            "Epoch: 13/20... Step: 3570... Loss: 1.1332... Val Loss: 1.3702\n",
            "Epoch: 13/20... Step: 3580... Loss: 1.1677... Val Loss: 1.3711\n",
            "Epoch: 13/20... Step: 3590... Loss: 1.1193... Val Loss: 1.3717\n",
            "Epoch: 13/20... Step: 3600... Loss: 1.1363... Val Loss: 1.3760\n",
            "Epoch: 13/20... Step: 3610... Loss: 1.1133... Val Loss: 1.3731\n",
            "Epoch: 13/20... Step: 3620... Loss: 1.1036... Val Loss: 1.3714\n",
            "Epoch: 14/20... Step: 3630... Loss: 1.1476... Val Loss: 1.3718\n",
            "Epoch: 14/20... Step: 3640... Loss: 1.1553... Val Loss: 1.3729\n",
            "Epoch: 14/20... Step: 3650... Loss: 1.1319... Val Loss: 1.3670\n",
            "Epoch: 14/20... Step: 3660... Loss: 1.0959... Val Loss: 1.3722\n",
            "Epoch: 14/20... Step: 3670... Loss: 1.0938... Val Loss: 1.3711\n",
            "Epoch: 14/20... Step: 3680... Loss: 1.1112... Val Loss: 1.3698\n",
            "Epoch: 14/20... Step: 3690... Loss: 1.1546... Val Loss: 1.3710\n",
            "Epoch: 14/20... Step: 3700... Loss: 1.0685... Val Loss: 1.3712\n",
            "Epoch: 14/20... Step: 3710... Loss: 1.1357... Val Loss: 1.3710\n",
            "Epoch: 14/20... Step: 3720... Loss: 1.1042... Val Loss: 1.3699\n",
            "Epoch: 14/20... Step: 3730... Loss: 1.1101... Val Loss: 1.3687\n",
            "Epoch: 14/20... Step: 3740... Loss: 1.0799... Val Loss: 1.3694\n",
            "Epoch: 14/20... Step: 3750... Loss: 1.1680... Val Loss: 1.3727\n",
            "Epoch: 14/20... Step: 3760... Loss: 1.1409... Val Loss: 1.3689\n",
            "Epoch: 14/20... Step: 3770... Loss: 1.0958... Val Loss: 1.3693\n",
            "Epoch: 14/20... Step: 3780... Loss: 1.1164... Val Loss: 1.3624\n",
            "Epoch: 14/20... Step: 3790... Loss: 1.1050... Val Loss: 1.3648\n",
            "Epoch: 14/20... Step: 3800... Loss: 1.1359... Val Loss: 1.3651\n",
            "Epoch: 14/20... Step: 3810... Loss: 1.1011... Val Loss: 1.3666\n",
            "Epoch: 14/20... Step: 3820... Loss: 1.1487... Val Loss: 1.3595\n",
            "Epoch: 14/20... Step: 3830... Loss: 1.1238... Val Loss: 1.3634\n",
            "Epoch: 14/20... Step: 3840... Loss: 1.1046... Val Loss: 1.3634\n",
            "Epoch: 14/20... Step: 3850... Loss: 1.1232... Val Loss: 1.3657\n",
            "Epoch: 14/20... Step: 3860... Loss: 1.1062... Val Loss: 1.3664\n",
            "Epoch: 14/20... Step: 3870... Loss: 1.1388... Val Loss: 1.3689\n",
            "Epoch: 14/20... Step: 3880... Loss: 1.0924... Val Loss: 1.3685\n",
            "Epoch: 14/20... Step: 3890... Loss: 1.1246... Val Loss: 1.3693\n",
            "Epoch: 14/20... Step: 3900... Loss: 1.1084... Val Loss: 1.3669\n",
            "Epoch: 15/20... Step: 3910... Loss: 1.0898... Val Loss: 1.3663\n",
            "Epoch: 15/20... Step: 3920... Loss: 1.1180... Val Loss: 1.3680\n",
            "Epoch: 15/20... Step: 3930... Loss: 1.0969... Val Loss: 1.3621\n",
            "Epoch: 15/20... Step: 3940... Loss: 1.1178... Val Loss: 1.3675\n",
            "Epoch: 15/20... Step: 3950... Loss: 1.1356... Val Loss: 1.3680\n",
            "Epoch: 15/20... Step: 3960... Loss: 1.0793... Val Loss: 1.3683\n",
            "Epoch: 15/20... Step: 3970... Loss: 1.0861... Val Loss: 1.3659\n",
            "Epoch: 15/20... Step: 3980... Loss: 1.0840... Val Loss: 1.3689\n",
            "Epoch: 15/20... Step: 3990... Loss: 1.0934... Val Loss: 1.3702\n",
            "Epoch: 15/20... Step: 4000... Loss: 1.1353... Val Loss: 1.3670\n",
            "Epoch: 15/20... Step: 4010... Loss: 1.0935... Val Loss: 1.3631\n",
            "Epoch: 15/20... Step: 4020... Loss: 1.0882... Val Loss: 1.3633\n",
            "Epoch: 15/20... Step: 4030... Loss: 1.0777... Val Loss: 1.3658\n",
            "Epoch: 15/20... Step: 4040... Loss: 1.1062... Val Loss: 1.3661\n",
            "Epoch: 15/20... Step: 4050... Loss: 1.1218... Val Loss: 1.3679\n",
            "Epoch: 15/20... Step: 4060... Loss: 1.0887... Val Loss: 1.3634\n",
            "Epoch: 15/20... Step: 4070... Loss: 1.1025... Val Loss: 1.3657\n",
            "Epoch: 15/20... Step: 4080... Loss: 1.1101... Val Loss: 1.3656\n",
            "Epoch: 15/20... Step: 4090... Loss: 1.0953... Val Loss: 1.3685\n",
            "Epoch: 15/20... Step: 4100... Loss: 1.0515... Val Loss: 1.3616\n",
            "Epoch: 15/20... Step: 4110... Loss: 1.1028... Val Loss: 1.3655\n",
            "Epoch: 15/20... Step: 4120... Loss: 1.1193... Val Loss: 1.3600\n",
            "Epoch: 15/20... Step: 4130... Loss: 1.1415... Val Loss: 1.3634\n",
            "Epoch: 15/20... Step: 4140... Loss: 1.1208... Val Loss: 1.3628\n",
            "Epoch: 15/20... Step: 4150... Loss: 1.1355... Val Loss: 1.3658\n",
            "Epoch: 15/20... Step: 4160... Loss: 1.0877... Val Loss: 1.3708\n",
            "Epoch: 15/20... Step: 4170... Loss: 1.1241... Val Loss: 1.3687\n",
            "Epoch: 15/20... Step: 4180... Loss: 1.0979... Val Loss: 1.3642\n",
            "Epoch: 16/20... Step: 4190... Loss: 1.0949... Val Loss: 1.3675\n",
            "Epoch: 16/20... Step: 4200... Loss: 1.1188... Val Loss: 1.3710\n",
            "Epoch: 16/20... Step: 4210... Loss: 1.1189... Val Loss: 1.3635\n",
            "Epoch: 16/20... Step: 4220... Loss: 1.1137... Val Loss: 1.3656\n",
            "Epoch: 16/20... Step: 4230... Loss: 1.0728... Val Loss: 1.3673\n",
            "Epoch: 16/20... Step: 4240... Loss: 1.0965... Val Loss: 1.3660\n",
            "Epoch: 16/20... Step: 4250... Loss: 1.0907... Val Loss: 1.3644\n",
            "Epoch: 16/20... Step: 4260... Loss: 1.1060... Val Loss: 1.3675\n",
            "Epoch: 16/20... Step: 4270... Loss: 1.0697... Val Loss: 1.3668\n",
            "Epoch: 16/20... Step: 4280... Loss: 1.0980... Val Loss: 1.3631\n",
            "Epoch: 16/20... Step: 4290... Loss: 1.0346... Val Loss: 1.3634\n",
            "Epoch: 16/20... Step: 4300... Loss: 1.0499... Val Loss: 1.3633\n",
            "Epoch: 16/20... Step: 4310... Loss: 1.0958... Val Loss: 1.3641\n",
            "Epoch: 16/20... Step: 4320... Loss: 1.0974... Val Loss: 1.3637\n",
            "Epoch: 16/20... Step: 4330... Loss: 1.1166... Val Loss: 1.3616\n",
            "Epoch: 16/20... Step: 4340... Loss: 1.0984... Val Loss: 1.3610\n",
            "Epoch: 16/20... Step: 4350... Loss: 1.0986... Val Loss: 1.3597\n",
            "Epoch: 16/20... Step: 4360... Loss: 1.1391... Val Loss: 1.3632\n",
            "Epoch: 16/20... Step: 4370... Loss: 1.0692... Val Loss: 1.3650\n",
            "Epoch: 16/20... Step: 4380... Loss: 1.0825... Val Loss: 1.3601\n",
            "Epoch: 16/20... Step: 4390... Loss: 1.1125... Val Loss: 1.3584\n",
            "Epoch: 16/20... Step: 4400... Loss: 1.0809... Val Loss: 1.3575\n",
            "Epoch: 16/20... Step: 4410... Loss: 1.0741... Val Loss: 1.3626\n",
            "Epoch: 16/20... Step: 4420... Loss: 1.0824... Val Loss: 1.3607\n",
            "Epoch: 16/20... Step: 4430... Loss: 1.0982... Val Loss: 1.3593\n",
            "Epoch: 16/20... Step: 4440... Loss: 1.0745... Val Loss: 1.3648\n",
            "Epoch: 16/20... Step: 4450... Loss: 1.0925... Val Loss: 1.3616\n",
            "Epoch: 16/20... Step: 4460... Loss: 1.1317... Val Loss: 1.3603\n",
            "Epoch: 17/20... Step: 4470... Loss: 1.0657... Val Loss: 1.3605\n",
            "Epoch: 17/20... Step: 4480... Loss: 1.1167... Val Loss: 1.3641\n",
            "Epoch: 17/20... Step: 4490... Loss: 1.0489... Val Loss: 1.3604\n",
            "Epoch: 17/20... Step: 4500... Loss: 1.0972... Val Loss: 1.3643\n",
            "Epoch: 17/20... Step: 4510... Loss: 1.0959... Val Loss: 1.3639\n",
            "Epoch: 17/20... Step: 4520... Loss: 1.0496... Val Loss: 1.3614\n",
            "Epoch: 17/20... Step: 4530... Loss: 1.0844... Val Loss: 1.3599\n",
            "Epoch: 17/20... Step: 4540... Loss: 1.0583... Val Loss: 1.3618\n",
            "Epoch: 17/20... Step: 4550... Loss: 1.0389... Val Loss: 1.3644\n",
            "Epoch: 17/20... Step: 4560... Loss: 1.0946... Val Loss: 1.3602\n",
            "Epoch: 17/20... Step: 4570... Loss: 1.0394... Val Loss: 1.3622\n",
            "Epoch: 17/20... Step: 4580... Loss: 1.0533... Val Loss: 1.3646\n",
            "Epoch: 17/20... Step: 4590... Loss: 1.0508... Val Loss: 1.3613\n",
            "Epoch: 17/20... Step: 4600... Loss: 1.0770... Val Loss: 1.3617\n",
            "Epoch: 17/20... Step: 4610... Loss: 1.0399... Val Loss: 1.3608\n",
            "Epoch: 17/20... Step: 4620... Loss: 1.0582... Val Loss: 1.3585\n",
            "Epoch: 17/20... Step: 4630... Loss: 1.0697... Val Loss: 1.3599\n",
            "Epoch: 17/20... Step: 4640... Loss: 1.1008... Val Loss: 1.3586\n",
            "Epoch: 17/20... Step: 4650... Loss: 1.0779... Val Loss: 1.3622\n",
            "Epoch: 17/20... Step: 4660... Loss: 1.0482... Val Loss: 1.3591\n",
            "Epoch: 17/20... Step: 4670... Loss: 1.0689... Val Loss: 1.3594\n",
            "Epoch: 17/20... Step: 4680... Loss: 1.0619... Val Loss: 1.3582\n",
            "Epoch: 17/20... Step: 4690... Loss: 1.0427... Val Loss: 1.3608\n",
            "Epoch: 17/20... Step: 4700... Loss: 1.0631... Val Loss: 1.3585\n",
            "Epoch: 17/20... Step: 4710... Loss: 1.0356... Val Loss: 1.3594\n",
            "Epoch: 17/20... Step: 4720... Loss: 1.0603... Val Loss: 1.3669\n",
            "Epoch: 17/20... Step: 4730... Loss: 1.0575... Val Loss: 1.3641\n",
            "Epoch: 17/20... Step: 4740... Loss: 1.0670... Val Loss: 1.3609\n",
            "Epoch: 18/20... Step: 4750... Loss: 1.0314... Val Loss: 1.3574\n",
            "Epoch: 18/20... Step: 4760... Loss: 1.0862... Val Loss: 1.3668\n",
            "Epoch: 18/20... Step: 4770... Loss: 1.0200... Val Loss: 1.3614\n",
            "Epoch: 18/20... Step: 4780... Loss: 1.0818... Val Loss: 1.3665\n",
            "Epoch: 18/20... Step: 4790... Loss: 1.0949... Val Loss: 1.3653\n",
            "Epoch: 18/20... Step: 4800... Loss: 1.0548... Val Loss: 1.3650\n",
            "Epoch: 18/20... Step: 4810... Loss: 1.0619... Val Loss: 1.3640\n",
            "Epoch: 18/20... Step: 4820... Loss: 1.0403... Val Loss: 1.3614\n",
            "Epoch: 18/20... Step: 4830... Loss: 1.0054... Val Loss: 1.3674\n",
            "Epoch: 18/20... Step: 4840... Loss: 1.0806... Val Loss: 1.3639\n",
            "Epoch: 18/20... Step: 4850... Loss: 1.0358... Val Loss: 1.3631\n",
            "Epoch: 18/20... Step: 4860... Loss: 1.0569... Val Loss: 1.3643\n",
            "Epoch: 18/20... Step: 4870... Loss: 1.0549... Val Loss: 1.3615\n",
            "Epoch: 18/20... Step: 4880... Loss: 1.0621... Val Loss: 1.3640\n",
            "Epoch: 18/20... Step: 4890... Loss: 1.0587... Val Loss: 1.3610\n",
            "Epoch: 18/20... Step: 4900... Loss: 1.0609... Val Loss: 1.3587\n",
            "Epoch: 18/20... Step: 4910... Loss: 1.0927... Val Loss: 1.3628\n",
            "Epoch: 18/20... Step: 4920... Loss: 1.0930... Val Loss: 1.3608\n",
            "Epoch: 18/20... Step: 4930... Loss: 1.0712... Val Loss: 1.3679\n",
            "Epoch: 18/20... Step: 4940... Loss: 1.0623... Val Loss: 1.3620\n",
            "Epoch: 18/20... Step: 4950... Loss: 1.0754... Val Loss: 1.3636\n",
            "Epoch: 18/20... Step: 4960... Loss: 1.0328... Val Loss: 1.3603\n",
            "Epoch: 18/20... Step: 4970... Loss: 1.0477... Val Loss: 1.3623\n",
            "Epoch: 18/20... Step: 4980... Loss: 1.0365... Val Loss: 1.3608\n",
            "Epoch: 18/20... Step: 4990... Loss: 1.0838... Val Loss: 1.3600\n",
            "Epoch: 18/20... Step: 5000... Loss: 1.0786... Val Loss: 1.3704\n",
            "Epoch: 18/20... Step: 5010... Loss: 1.0675... Val Loss: 1.3692\n",
            "Epoch: 18/20... Step: 5020... Loss: 1.0501... Val Loss: 1.3664\n",
            "Epoch: 19/20... Step: 5030... Loss: 1.0468... Val Loss: 1.3635\n",
            "Epoch: 19/20... Step: 5040... Loss: 1.0670... Val Loss: 1.3706\n",
            "Epoch: 19/20... Step: 5050... Loss: 1.0138... Val Loss: 1.3685\n",
            "Epoch: 19/20... Step: 5060... Loss: 1.0467... Val Loss: 1.3652\n",
            "Epoch: 19/20... Step: 5070... Loss: 1.0255... Val Loss: 1.3661\n",
            "Epoch: 19/20... Step: 5080... Loss: 1.0287... Val Loss: 1.3672\n",
            "Epoch: 19/20... Step: 5090... Loss: 1.0266... Val Loss: 1.3659\n",
            "Epoch: 19/20... Step: 5100... Loss: 1.0483... Val Loss: 1.3673\n",
            "Epoch: 19/20... Step: 5110... Loss: 1.0436... Val Loss: 1.3740\n",
            "Epoch: 19/20... Step: 5120... Loss: 1.0363... Val Loss: 1.3706\n",
            "Epoch: 19/20... Step: 5130... Loss: 1.0345... Val Loss: 1.3677\n",
            "Epoch: 19/20... Step: 5140... Loss: 1.0225... Val Loss: 1.3697\n",
            "Epoch: 19/20... Step: 5150... Loss: 1.0682... Val Loss: 1.3633\n",
            "Epoch: 19/20... Step: 5160... Loss: 1.0438... Val Loss: 1.3653\n",
            "Epoch: 19/20... Step: 5170... Loss: 1.0358... Val Loss: 1.3650\n",
            "Epoch: 19/20... Step: 5180... Loss: 1.0413... Val Loss: 1.3638\n",
            "Epoch: 19/20... Step: 5190... Loss: 1.0474... Val Loss: 1.3644\n",
            "Epoch: 19/20... Step: 5200... Loss: 1.0556... Val Loss: 1.3683\n",
            "Epoch: 19/20... Step: 5210... Loss: 1.0256... Val Loss: 1.3721\n",
            "Epoch: 19/20... Step: 5220... Loss: 1.0446... Val Loss: 1.3664\n",
            "Epoch: 19/20... Step: 5230... Loss: 1.0561... Val Loss: 1.3652\n",
            "Epoch: 19/20... Step: 5240... Loss: 1.0737... Val Loss: 1.3679\n",
            "Epoch: 19/20... Step: 5250... Loss: 1.0214... Val Loss: 1.3698\n",
            "Epoch: 19/20... Step: 5260... Loss: 1.0550... Val Loss: 1.3667\n",
            "Epoch: 19/20... Step: 5270... Loss: 1.0333... Val Loss: 1.3656\n",
            "Epoch: 19/20... Step: 5280... Loss: 1.0241... Val Loss: 1.3755\n",
            "Epoch: 19/20... Step: 5290... Loss: 1.0173... Val Loss: 1.3711\n",
            "Epoch: 19/20... Step: 5300... Loss: 1.0334... Val Loss: 1.3709\n",
            "Epoch: 20/20... Step: 5310... Loss: 1.0322... Val Loss: 1.3655\n",
            "Epoch: 20/20... Step: 5320... Loss: 1.0556... Val Loss: 1.3768\n",
            "Epoch: 20/20... Step: 5330... Loss: 1.0171... Val Loss: 1.3723\n",
            "Epoch: 20/20... Step: 5340... Loss: 0.9989... Val Loss: 1.3725\n",
            "Epoch: 20/20... Step: 5350... Loss: 1.0101... Val Loss: 1.3709\n",
            "Epoch: 20/20... Step: 5360... Loss: 1.0004... Val Loss: 1.3726\n",
            "Epoch: 20/20... Step: 5370... Loss: 1.0393... Val Loss: 1.3705\n",
            "Epoch: 20/20... Step: 5380... Loss: 1.0237... Val Loss: 1.3671\n",
            "Epoch: 20/20... Step: 5390... Loss: 1.0307... Val Loss: 1.3733\n",
            "Epoch: 20/20... Step: 5400... Loss: 1.0107... Val Loss: 1.3757\n",
            "Epoch: 20/20... Step: 5410... Loss: 1.0219... Val Loss: 1.3690\n",
            "Epoch: 20/20... Step: 5420... Loss: 0.9941... Val Loss: 1.3748\n",
            "Epoch: 20/20... Step: 5430... Loss: 1.0348... Val Loss: 1.3677\n",
            "Epoch: 20/20... Step: 5440... Loss: 1.0313... Val Loss: 1.3719\n",
            "Epoch: 20/20... Step: 5450... Loss: 1.0461... Val Loss: 1.3764\n",
            "Epoch: 20/20... Step: 5460... Loss: 1.0297... Val Loss: 1.3697\n",
            "Epoch: 20/20... Step: 5470... Loss: 1.0569... Val Loss: 1.3698\n",
            "Epoch: 20/20... Step: 5480... Loss: 1.0229... Val Loss: 1.3689\n",
            "Epoch: 20/20... Step: 5490... Loss: 1.0127... Val Loss: 1.3748\n",
            "Epoch: 20/20... Step: 5500... Loss: 1.0226... Val Loss: 1.3731\n",
            "Epoch: 20/20... Step: 5510... Loss: 1.0478... Val Loss: 1.3729\n",
            "Epoch: 20/20... Step: 5520... Loss: 1.0315... Val Loss: 1.3725\n",
            "Epoch: 20/20... Step: 5530... Loss: 1.0590... Val Loss: 1.3731\n",
            "Epoch: 20/20... Step: 5540... Loss: 1.0215... Val Loss: 1.3718\n",
            "Epoch: 20/20... Step: 5550... Loss: 1.0009... Val Loss: 1.3696\n",
            "Epoch: 20/20... Step: 5560... Loss: 1.0262... Val Loss: 1.3755\n",
            "Epoch: 20/20... Step: 5570... Loss: 1.0367... Val Loss: 1.3773\n",
            "Epoch: 20/20... Step: 5580... Loss: 1.0887... Val Loss: 1.3726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpqN3Mi5Yde0"
      },
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_20_epoch.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.num_hidden,\n",
        "              'n_layers': net.num_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bd6owfwdwFO"
      },
      "source": [
        "# Top K prediciton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w1mv7PBYde0"
      },
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        # print(x)\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        # print(x)\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "        # print('out.shape: ', out.shape)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        # print('P shape: ', p.shape)\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            # print('p:',p.shape)\n",
        "            # print('top_ch',top_ch)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaaSOf1DeXCl",
        "outputId": "c8ca01af-1265-4ad6-daa1-da277e25e061"
      },
      "source": [
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    # chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    char, h = predict(net, 'T', h, top_k=5)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[77]]\n",
            "[[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "   0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]]\n",
            "out.shape:  torch.Size([1, 83])\n",
            "P shape:  torch.Size([1, 83])\n",
            "p: torch.Size([1, 5])\n",
            "top_ch tensor([[44, 72, 63, 57, 67]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNrJEpzEeS0z"
      },
      "source": [
        "def sample(net, size, prime='The', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    print(chars)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(1500):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHkayT6yecl_",
        "outputId": "992d5edf-4ac1-4e6f-c72e-375867bb8bf8"
      },
      "source": [
        "print(sample(net, 1000, prime='love', top_k=5))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['l', 'o', 'v', 'e', '!']\n",
            "love! It's night.\n",
            "I see you are, and your decision and all meanings.\"\n",
            "\n",
            "These letter in the wedding, when she went up to the stopping out of the\n",
            "country, taking his hand, sent him to secort, which had so set, to\n",
            "service him abone to too with the peasants, but he had stopped him\n",
            "with the baby. But when the princess and Anna had been taking off the\n",
            "conversation with her husband with a bad horse, and the sound of talk of the\n",
            "presching chair of the short head and still maked his hand and shook his\n",
            "head on the room.\n",
            "\n",
            "\n",
            "\n",
            " Chapter 15\n",
            "\n",
            "\n",
            "At this moment it seemed to him that some soul to see the proofs without the\n",
            "stalls. He was not a cheat former in the marsh; and took\n",
            "off his chair, and studging the sun of some service. The stern\n",
            "way there was another word, the chief seconds of this considerations\n",
            "of her husband, she was not made and make up his heart, her shame of the sun was stood at\n",
            "her, and again she did not know, that this was harrer of happiness and was she too,\n",
            "to servant were all of them.\n",
            "\n",
            "\"I don't know why he had said to you,\" answered Lidia,\n",
            "\"but and that's new them all it.\"\n",
            "\n",
            "\"I am very glad to see.... You should like to meet you,\" and with a\n",
            "mongrasing confusion on her honor and simple his son, and so\n",
            "as to go on like a babor smoked.\n",
            "\n",
            "As his face was already an hour of sorrow, and they was now so all\n",
            "of all, and had busy himself what was a child into a little great deal on a divid, and\n",
            "with a legs subbounts in a blacking of the carriage, hadgened the distonstrous\n",
            "thoughts ordered\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l39zoeVfgthi"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}