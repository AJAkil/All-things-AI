{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d064147f-9d83-4bd9-a55f-3dbed043ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from utils import translate_sentence, bleu, load_checkpoint, save_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d1242038-46b0-41a4-b429-417ce84a8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "spacy_ger = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "828c6225-8350-41b8-8814-2a312224605a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_eng(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "def tokenize_ger(text):\n",
    "    return [tok.text for tok in spacy_ger.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3c074c7d-1c1e-413f-9f5f-b0b16e89dcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "english = Field(tokenize=tokenize_eng, lower=True, init_token='<sos>', eos_token='<eos>')\n",
    "german = Field(tokenize=tokenize_ger, lower=True, init_token='<sos>', eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d33e753f-aeab-474a-a45e-eb1a57097789",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(german, english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8f1aa647-2f51-4503-aa42-0b85626d550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "062f713c-f616-4521-82bc-05df383ab9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, drop_prob):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=drop_prob)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (seq_length, N) ---> has N batches each of seq_length\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size) ---> Each of the word has an embedding of size in embedding_size\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedding)\n",
    "        # output shape: [seq_length, batch_size, hid_dim * n_directions ]\n",
    "        # hidden shape: [n_layers * n_direction, batch_size, hid_dim ]\n",
    "        # cell state shape: [n_layers * n_direction, batch_size, hid_dim ]\n",
    "        \n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f27234eb-01b5-4250-a021-d68227cbea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, drop_prob):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=drop_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        #x = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        # x = [1, batch Size]\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        \n",
    "        # embedding = [1, batch size, embedding size]\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(embedding, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        predictions = self.fc(output)\n",
    "        \n",
    "        # predictions = [1, batch_size, output_dim]\n",
    "        \n",
    "        predictions = predictions.squeeze(0)\n",
    "        \n",
    "        # predictions = [batch_size, output_dim]\n",
    "        \n",
    "        return predictions, hidden, cell\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "65ee5245-e9c0-4a6f-8732-ba030f6904f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hidden_size == decoder.hidden_size, \\\n",
    "            \"Hidden size of encoder and decoder does not match\"\n",
    "        assert encoder.num_layers == decoder.num_layers, \\\n",
    "            \"Num layers of encoder and decoder does not match\"\n",
    "        \n",
    "    def forward(self, source, target, teacher_forcing_ratio = 0.5):\n",
    "         #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = source.shape[1]\n",
    "        trg_len = target.shape[0]\n",
    "        target_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        outputs = torch.zeros(trg_len, batch_size, target_vocab_size).to(device)\n",
    "        \n",
    "        hidden, cell = self.encoder(source)\n",
    "        \n",
    "        # get the first token for the decoder <sos> across all batches\n",
    "        x = target[0, :] \n",
    "        \n",
    "        for t in range(trg_len):\n",
    "            \n",
    "            # insert input token embedding, previous hidden and previous cell states\n",
    "            # receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            # output  = [batch size, output_dim]\n",
    "            \n",
    "            # place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            # get the best guess using argmax for each of the batch\n",
    "            top_one = output.argmax(1)\n",
    "            \n",
    "            # set the next input conditioning on teacher forcing usage or not\n",
    "            x = target[t] if random.random() < teacher_forcing_ratio else top_one\n",
    "        \n",
    "        return outputs\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0020680d-9c61-4fd9-b531-3fa02f86b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "num_epochs = 20\n",
    "learing_rate = 0.01\n",
    "batch_size = 64\n",
    "\n",
    "# Model Hyperparameters\n",
    "load_model = False\n",
    "device = torch.device('cpu')\n",
    "input_size_encoder = len(german.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "# TensorBoard\n",
    "writer = SummaryWriter(f'runs/loss_plot')\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "44b11379-627c-49cc-928a-853272682b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_iterator, validation_iterator, test_iterator = BucketIterator.splits(\n",
    "            (train_data, validation_data, test_data),\n",
    "                batch_size=batch_size, \n",
    "                sort_within_batch=True,\n",
    "                sort_key = lambda x: len(x.src), \n",
    "                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "050e1064-800b-49f1-a7e7-e52ceb33003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the model\n",
    "encoder = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout).to(device)\n",
    "decoder = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout).to(device)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3cbac97e-509d-4026-90c5-7ccb28254fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.embedding.weight Parameter containing:\n",
      "tensor([[-1.3564,  1.1339,  0.7082,  ..., -0.6823, -1.6562, -0.7343],\n",
      "        [ 0.5285, -0.6477,  1.0205,  ..., -0.0385, -0.5533,  0.3937],\n",
      "        [-0.8782, -1.7481,  1.0477,  ..., -0.7103,  0.1289, -0.4427],\n",
      "        ...,\n",
      "        [-0.6792,  0.6214,  2.3403,  ..., -1.3644,  1.1562, -0.3353],\n",
      "        [-0.0353, -0.0175, -0.3030,  ...,  1.1515, -0.9493, -0.3736],\n",
      "        [-0.1261,  0.2769, -0.4705,  ..., -0.6186, -0.1904, -0.7831]],\n",
      "       requires_grad=True)\n",
      "encoder.lstm.weight_ih_l0 Parameter containing:\n",
      "tensor([[-0.0076,  0.0171, -0.0194,  ..., -0.0186,  0.0142,  0.0195],\n",
      "        [-0.0063, -0.0243, -0.0153,  ...,  0.0199, -0.0009, -0.0055],\n",
      "        [ 0.0095,  0.0291, -0.0268,  ..., -0.0054,  0.0063,  0.0257],\n",
      "        ...,\n",
      "        [ 0.0118, -0.0158,  0.0153,  ..., -0.0259,  0.0174, -0.0072],\n",
      "        [-0.0302,  0.0168,  0.0289,  ..., -0.0068,  0.0236, -0.0027],\n",
      "        [ 0.0101,  0.0209, -0.0265,  ..., -0.0241, -0.0225,  0.0019]],\n",
      "       requires_grad=True)\n",
      "encoder.lstm.weight_hh_l0 Parameter containing:\n",
      "tensor([[-0.0301,  0.0139,  0.0139,  ..., -0.0020,  0.0269, -0.0074],\n",
      "        [-0.0056,  0.0151,  0.0256,  ...,  0.0180, -0.0057,  0.0295],\n",
      "        [ 0.0061,  0.0202, -0.0025,  ..., -0.0107, -0.0310,  0.0294],\n",
      "        ...,\n",
      "        [ 0.0034,  0.0273,  0.0194,  ...,  0.0058, -0.0073,  0.0063],\n",
      "        [-0.0278,  0.0274, -0.0074,  ...,  0.0264, -0.0312, -0.0019],\n",
      "        [-0.0170, -0.0013,  0.0146,  ...,  0.0119,  0.0096, -0.0190]],\n",
      "       requires_grad=True)\n",
      "encoder.lstm.bias_ih_l0 Parameter containing:\n",
      "tensor([-0.0157, -0.0273, -0.0054,  ..., -0.0210,  0.0021,  0.0098],\n",
      "       requires_grad=True)\n",
      "encoder.lstm.bias_hh_l0 Parameter containing:\n",
      "tensor([-0.0146, -0.0109,  0.0069,  ..., -0.0268, -0.0144,  0.0170],\n",
      "       requires_grad=True)\n",
      "encoder.lstm.weight_ih_l1 Parameter containing:\n",
      "tensor([[-0.0029,  0.0213, -0.0300,  ..., -0.0013,  0.0270,  0.0069],\n",
      "        [-0.0308, -0.0130,  0.0111,  ..., -0.0208, -0.0270,  0.0016],\n",
      "        [ 0.0307, -0.0290, -0.0191,  ..., -0.0162, -0.0288,  0.0079],\n",
      "        ...,\n",
      "        [-0.0074, -0.0155, -0.0093,  ...,  0.0309,  0.0102,  0.0273],\n",
      "        [ 0.0312,  0.0186, -0.0190,  ...,  0.0167, -0.0239,  0.0135],\n",
      "        [ 0.0205,  0.0033,  0.0116,  ...,  0.0052,  0.0248,  0.0075]],\n",
      "       requires_grad=True)\n",
      "encoder.lstm.weight_hh_l1 Parameter containing:\n",
      "tensor([[ 0.0027, -0.0035,  0.0184,  ...,  0.0132,  0.0170, -0.0215],\n",
      "        [-0.0072,  0.0088, -0.0144,  ..., -0.0013, -0.0034,  0.0235],\n",
      "        [-0.0285, -0.0132, -0.0247,  ...,  0.0220,  0.0257,  0.0050],\n",
      "        ...,\n",
      "        [ 0.0270,  0.0222,  0.0262,  ...,  0.0055,  0.0216,  0.0278],\n",
      "        [-0.0226, -0.0236,  0.0301,  ..., -0.0050,  0.0088,  0.0168],\n",
      "        [-0.0055,  0.0228,  0.0011,  ..., -0.0180, -0.0179,  0.0204]],\n",
      "       requires_grad=True)\n",
      "encoder.lstm.bias_ih_l1 Parameter containing:\n",
      "tensor([-0.0071,  0.0009, -0.0295,  ..., -0.0279, -0.0042, -0.0005],\n",
      "       requires_grad=True)\n",
      "encoder.lstm.bias_hh_l1 Parameter containing:\n",
      "tensor([-0.0008,  0.0222,  0.0069,  ..., -0.0147, -0.0222, -0.0037],\n",
      "       requires_grad=True)\n",
      "decoder.embedding.weight Parameter containing:\n",
      "tensor([[-1.2804,  1.6579, -0.5346,  ..., -0.3934,  0.3953,  1.9959],\n",
      "        [-0.9261, -0.6490,  2.0304,  ..., -0.0251, -0.3099,  0.6036],\n",
      "        [ 0.3017,  0.5210, -1.0584,  ..., -0.3636,  1.1651,  0.7144],\n",
      "        ...,\n",
      "        [-0.0704, -0.8141, -0.2210,  ...,  1.7363,  0.4597, -1.2082],\n",
      "        [-0.5655,  0.0551, -1.3136,  ...,  0.9990,  0.0756, -0.2591],\n",
      "        [-0.2670,  1.3009,  0.2813,  ..., -0.6580,  0.4608,  1.1840]],\n",
      "       requires_grad=True)\n",
      "decoder.lstm.weight_ih_l0 Parameter containing:\n",
      "tensor([[-0.0015, -0.0200, -0.0051,  ..., -0.0196,  0.0051, -0.0280],\n",
      "        [-0.0073,  0.0295,  0.0114,  ...,  0.0309,  0.0231, -0.0134],\n",
      "        [ 0.0201,  0.0311,  0.0009,  ...,  0.0092,  0.0026,  0.0161],\n",
      "        ...,\n",
      "        [-0.0249, -0.0108, -0.0110,  ...,  0.0271,  0.0148, -0.0217],\n",
      "        [-0.0030,  0.0029,  0.0004,  ...,  0.0174,  0.0074,  0.0113],\n",
      "        [-0.0127, -0.0306, -0.0164,  ...,  0.0211,  0.0101,  0.0290]],\n",
      "       requires_grad=True)\n",
      "decoder.lstm.weight_hh_l0 Parameter containing:\n",
      "tensor([[ 1.6860e-02, -2.1937e-02, -1.0761e-02,  ...,  2.4695e-02,\n",
      "         -1.5580e-02, -2.1248e-02],\n",
      "        [ 1.5712e-02, -2.3027e-02,  2.3389e-02,  ...,  7.6690e-03,\n",
      "         -3.0063e-02,  2.6230e-02],\n",
      "        [-3.0874e-02,  1.4117e-02, -9.6370e-05,  ...,  3.0902e-02,\n",
      "          9.6804e-03, -2.0159e-02],\n",
      "        ...,\n",
      "        [-2.9822e-02,  2.7699e-02, -8.6816e-03,  ...,  1.5352e-03,\n",
      "          8.5176e-03, -5.0877e-03],\n",
      "        [ 1.8322e-02, -1.8224e-02,  2.4838e-03,  ..., -2.6312e-03,\n",
      "         -2.6011e-02, -1.9216e-02],\n",
      "        [-1.2530e-02, -2.2831e-02, -3.7250e-03,  ...,  8.4820e-03,\n",
      "         -2.9717e-02, -3.3771e-03]], requires_grad=True)\n",
      "decoder.lstm.bias_ih_l0 Parameter containing:\n",
      "tensor([-0.0284,  0.0285, -0.0175,  ..., -0.0280,  0.0025, -0.0089],\n",
      "       requires_grad=True)\n",
      "decoder.lstm.bias_hh_l0 Parameter containing:\n",
      "tensor([ 0.0298,  0.0258, -0.0013,  ..., -0.0169,  0.0033,  0.0034],\n",
      "       requires_grad=True)\n",
      "decoder.lstm.weight_ih_l1 Parameter containing:\n",
      "tensor([[-0.0187, -0.0092,  0.0016,  ..., -0.0226, -0.0215, -0.0289],\n",
      "        [ 0.0168,  0.0002,  0.0238,  ...,  0.0207, -0.0234, -0.0183],\n",
      "        [-0.0266, -0.0183,  0.0186,  ...,  0.0218, -0.0229, -0.0097],\n",
      "        ...,\n",
      "        [-0.0002,  0.0107, -0.0158,  ...,  0.0102, -0.0108,  0.0306],\n",
      "        [-0.0228, -0.0174,  0.0099,  ...,  0.0029, -0.0175,  0.0112],\n",
      "        [-0.0099,  0.0086,  0.0270,  ..., -0.0258,  0.0058, -0.0095]],\n",
      "       requires_grad=True)\n",
      "decoder.lstm.weight_hh_l1 Parameter containing:\n",
      "tensor([[-0.0228, -0.0234,  0.0272,  ...,  0.0129, -0.0162, -0.0198],\n",
      "        [ 0.0285,  0.0061, -0.0194,  ..., -0.0075, -0.0084, -0.0103],\n",
      "        [ 0.0131, -0.0291,  0.0182,  ..., -0.0082,  0.0055, -0.0119],\n",
      "        ...,\n",
      "        [-0.0059,  0.0102, -0.0053,  ...,  0.0036, -0.0107, -0.0210],\n",
      "        [-0.0061, -0.0076, -0.0169,  ...,  0.0118, -0.0115,  0.0272],\n",
      "        [-0.0126,  0.0294,  0.0109,  ...,  0.0257, -0.0047, -0.0085]],\n",
      "       requires_grad=True)\n",
      "decoder.lstm.bias_ih_l1 Parameter containing:\n",
      "tensor([-0.0295,  0.0241, -0.0024,  ...,  0.0129,  0.0188,  0.0018],\n",
      "       requires_grad=True)\n",
      "decoder.lstm.bias_hh_l1 Parameter containing:\n",
      "tensor([-0.0024,  0.0092,  0.0234,  ...,  0.0245, -0.0092, -0.0221],\n",
      "       requires_grad=True)\n",
      "decoder.fc.weight Parameter containing:\n",
      "tensor([[ 0.0226, -0.0059, -0.0135,  ..., -0.0006, -0.0227,  0.0152],\n",
      "        [ 0.0174, -0.0241,  0.0238,  ...,  0.0018, -0.0251, -0.0225],\n",
      "        [ 0.0295, -0.0275,  0.0186,  ..., -0.0024, -0.0305, -0.0082],\n",
      "        ...,\n",
      "        [ 0.0029,  0.0011,  0.0276,  ...,  0.0050, -0.0144, -0.0012],\n",
      "        [ 0.0294, -0.0068, -0.0112,  ..., -0.0055,  0.0033,  0.0058],\n",
      "        [-0.0182, -0.0260, -0.0164,  ..., -0.0156, -0.0119,  0.0207]],\n",
      "       requires_grad=True)\n",
      "decoder.fc.bias Parameter containing:\n",
      "tensor([ 0.0097, -0.0312,  0.0208,  ..., -0.0126,  0.0119, -0.0283],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6018309b-255d-4d8e-b672-fffab0729976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore the padding tokens by loss function\n",
    "pad_idx = english.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a14aaa80-6b63-4866-8db2-94664a70195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    load_checkpoint(torch.load('seq2seq_checkpoint.pth.ptar', model, optimizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "71c8ab31-2686-49bd-b9cd-76fd703082fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "55e8323e-40bc-4b0f-8adc-f165b0e80dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip, writer):\n",
    "    \n",
    "    # take the model to training mode(activates the dropout and batchnorm)\n",
    "    model.train()\n",
    "    \n",
    "    # to track the loss in each epoch\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(iterator):\n",
    "        \n",
    "        # src = [src_len, batch_size]\n",
    "        # trg = [trg_len, batch_size]\n",
    "        \n",
    "        src = batch.src.to(device)\n",
    "        trg = batch.trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        # trg = [(trg len - 1) * batch size]\n",
    "        # output = [(trg len - 1) * batch size, output_dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        #writer.add_scaler('Training Loss', loss, global_step=step)\n",
    "        #step += 1\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    # we return the loss per epoch for the training\n",
    "    return epoch_loss/len(iterator)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1ed0f65b-9519-48ef-ad62-62582b80575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for batch_index, batch in enumerate(iterator):\n",
    "            \n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "            \n",
    "            output = model(src, trg, 0)\n",
    "            \n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            \n",
    "        return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5d3ddef9-ffe7-42e6-a570-2f4d3c4fb4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7d90b8be-69ac-4914-9541-2c63a99a76db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-98-eacd67ab7dd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mepoch_mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_secs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepoch_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_valid_loss\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mbest_valid_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'seq2seq_model.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'valid_loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "num_epochs = 10\n",
    "clip = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, clip, writer)\n",
    "    validation_loss = evaluate(model, validation_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if validation_loss < best_valid_loss :\n",
    "        best_valid_loss = validation_loss\n",
    "        torch.save(model.state_dict(), 'seq2seq_model.pt')\n",
    "    \n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361e1b6e-69d0-4b90-8e51-97e77928291f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('seq2seq_model.pt'))\n",
    "\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d3c42-2dd6-4b29-87bc-851d504d5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = bleu(test_data[1:100], model, german, english, device)\n",
    "print(f\"Bleu score {score*100:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
